{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set_theme()\n",
    "plt.rcParams.update({'figure.facecolor':'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and do basic formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed_sensor_dwd_train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# convert pressure to hPa\n",
    "df['pressure_sensors'] = df['pressure_sensors'] / 100\n",
    "df['pressure_std'] = df['pressure_std'] / 100\n",
    "\n",
    "# add sensor IDs\n",
    "df_location = df.groupby(['lat', 'lon']).count().reset_index()[['lat', 'lon']]\n",
    "df_location['location_id'] = df_location.index+1\n",
    "df = df.merge(df_location, on=['lat', 'lon'], how='left')\n",
    "\n",
    "# define lists with columns\n",
    "no_data_cols = ['location_id', 'timestamp', 'city', 'lat', 'lon']\n",
    "sc_cols = sorted(['PM10', 'PM2p5', 'PM10_std', 'PM2p5_std', 'pressure_sensors', 'temperature_sensors', 'humidity_sensors', 'pressure_std', 'temperature_std', 'humidity_std'])\n",
    "sc_cols_wo_std = [col for col in sc_cols if 'std' not in col]\n",
    "dwd_cols = sorted([col for col in df.columns if (col not in no_data_cols and col not in sc_cols)])\n",
    "std_cols = [col for col in sc_cols if 'std' in col]\n",
    "data_cols_wo_std = sc_cols_wo_std + dwd_cols\n",
    "data_cols = sc_cols + dwd_cols\n",
    "\n",
    "# reorganize columns: first non-data columns, then sorted data columns\n",
    "df = df.reindex(columns=no_data_cols + sc_cols + dwd_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save assignment of sensor_id to coordinates\n",
    "location_id_assignment = pd.DataFrame(\n",
    "    data={\n",
    "        'location_id': df['location_id'].unique()\n",
    "    }\n",
    ")\n",
    "for l in ['lat', 'lon']:\n",
    "    location_id_assignment[l] = location_id_assignment.apply(lambda x: df.loc[df['location_id']==x['location_id'], l].iloc[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of missing values, zeros and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the whole sc dataset\n",
    "df[sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the sc dataset for Frankfurt\n",
    "df[df['city']=='Frankfurt'][sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the sc dataset for Bremen\n",
    "df[df['city']=='Bremen'][sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM10: Mean is almost double of the 75th percentile -> Outliers raise the mean extremely </br>\n",
    "PM2.5: similar to PM10, but less extreme </br>\n",
    "humidity: al values (mean, 25th, 50th and 75th percentile) seem to be very large, the max value is above 100, what doesn't make any sense </br>\n",
    "pressure: assuming the units are Pa (1 bar = 100.000 Pa): min value is below 100 -> unrealistic, max value is also unrealistic (more than 60 bar) </br>\n",
    "temperature: std seems very high (54 Â°C), min and max value are unrealistic </br>\n",
    " </br>\n",
    " Bremen vs. Frankfurt </br>\n",
    " PM10 and PM2.5: std for Bremen is double of std for Frankfurt </br>\n",
    " humidity: 50th percentile of Bremen is already 99.9 % what seems quite high\n",
    " pressure and temperature: no obvious unrealistic observations besides the min and max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"missing values in each column\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].isna().sum()} ({round(df[col].isna().sum() / df.shape[0] * 100, 1)} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"value '0' in each column\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[df[col]==0][col].count()} ({round(df[df[col]==0][col].count() / df.shape[0] * 100, 1)} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan_and_0s(df: pd.DataFrame, cols: list = None) -> pd.DataFrame:\n",
    "    \"\"\"Counts zeros and nans per column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to search for zeros and nans.\n",
    "        cols (list, optional): List of columns, if no columns are specified all will be used. Defaults to None.\n",
    "        thresholds (dict, optional): Thresholds for further . Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing counts of zeros and nans.\n",
    "    \"\"\"\n",
    "    # use all columns af none were defined\n",
    "    if cols == None:\n",
    "        cols=df.columns\n",
    "    # make a new dataframe and put the defined column names in the first column\n",
    "    df_nan_0 = pd.DataFrame()\n",
    "    df_nan_0['data'] = cols\n",
    "    # calculate missing values and zeros as absolute value and share \n",
    "    df_nan_0['missing_values'] = [df[col].isna().sum() for col in cols]\n",
    "    df_nan_0['missing_values_share'] = [df[col].isna().sum() / df.shape[0] * 100 for col in cols]\n",
    "    df_nan_0['0_values'] = [df[df[col]==0][col].count() for col in cols]\n",
    "    df_nan_0['0_values_share'] = [df[df[col]==0][col].count() / df.shape[0] * 100 for col in cols]\n",
    "\n",
    "    # transpose the dataframe and use the original column names as column names\n",
    "    df_nan_0 = df_nan_0.set_index('data').T.reset_index()\n",
    "    df_nan_0.columns = [name if i>0 else 'metric' for i, name in enumerate(df_nan_0.columns)]\n",
    "    return df_nan_0\n",
    "\n",
    "\n",
    "# find missing values and zeros in the sc dataset\n",
    "df_data_analysis = count_nan_and_0s(df, data_cols)\n",
    "df_data_analysis.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics and columns to plot\n",
    "metrics = [\"missing_values_share\", \"0_values_share\"]\n",
    "ys = list(df_data_analysis.columns)\n",
    "ys.remove('metric')\n",
    "\n",
    "# define size of subplot\n",
    "columns = 4\n",
    "rows = int(np.ceil((len(df_data_analysis.columns) - 1) / columns))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "plt.suptitle(\"Data analysis of missing values and zeros\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(ys):\n",
    "        \n",
    "            # create a bar for each metric defined above for a column of ys list\n",
    "            sns.barplot(data=df_data_analysis[df_data_analysis['metric'].isin(metrics)], x='metric', y=ys[col + row * columns], ax=ax[row][col])\n",
    "            # set ylim to [0, 100] as we are plotting percentages\n",
    "            ax[row][col].set_ylim([0, 100])\n",
    "            # put the percentage above each plotted bar\n",
    "            ax[row][col].bar_label(ax[row][col].containers[0], fmt='%.1f')\n",
    "            # set the x, y and x-tick labels\n",
    "            ax[row][col].set_xlabel(\"\")\n",
    "            ax[row][col].set_ylabel(\"Share of values in %\")\n",
    "            ax[row][col].set_xticklabels(labels=[\"Missing values\", \"Zeros\"])\n",
    "            # use the column name with slight changes as subplot name\n",
    "            title = f\"{ys[col + row * columns]}\".replace('_', ' ').replace('std', 'std. dev.').replace('2p5', '2.5').capitalize()\n",
    "            ax[row][col].set_title(title, fontsize = 15);\n",
    "        else:\n",
    "            # delete not needed subplots\n",
    "            fig.delaxes(ax[row][col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to plot\n",
    "ys = data_cols_wo_std\n",
    "\n",
    "# define size of subplot\n",
    "columns = 3\n",
    "rows = int(np.ceil((len(ys)) / columns))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "plt.suptitle(\"Outlier analysis\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .7, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(ys):\n",
    "\n",
    "            # create a bar for each metric defined above for a column of ys list\n",
    "            sns.scatterplot(data=df, x='timestamp', y=ys[col + row * columns], ax=ax[row][col], alpha=.3)\n",
    "            # set the x, y and x-tick labels\n",
    "            ax[row][col].set_xlabel(ax[row][col].get_xlabel().capitalize())\n",
    "            ax[row][col].set_ylabel(ax[row][col].get_ylabel().capitalize())\n",
    "            # use the column name with slight changes as subplot name\n",
    "            title = f\"{ys[col + row * columns]}\".replace('_', ' ').replace('std', 'std. dev.').replace('2p5', '2.5').capitalize()\n",
    "            ax[row][col].set_title(title, fontsize = 15)\n",
    "            ax[row][col].tick_params(labelrotation=90)\n",
    "        else:\n",
    "            # delete not needed subplots\n",
    "            fig.delaxes(ax[row][col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few outliers in humidity, pressure and temperature which can be dropped by setting thresholds. </br>\n",
    "For PM10 and PM2.5 it is less obvious as the data is scattered all over the possible range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete unrealistic values and outliers for environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard thresholds based on physical estimations\n",
    "We can first have a look at the extreme values measured by Deutscher Wetterdienst to get an impression what range of values is realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['humidity_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['humidity_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['humidity_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['pressure_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['pressure_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['pressure_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['temperature_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['temperature_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['temperature_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lower and upper threshold\n",
    "thresholds_env = {\n",
    "    'humidity_sensors': (15, 100),\n",
    "    'pressure_sensors': (960, 1050),\n",
    "    'temperature_sensors': (-20, 60),\n",
    "}\n",
    "\n",
    "def del_hard_thresholds_env(df, thresholds_env=thresholds_env):\n",
    "    # delete values below lower and above upper threshold\n",
    "    for col, thresh in thresholds_env.items():\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df.iloc[df[col] <= thresh[0], list(df.columns).index(col)] = np.nan\n",
    "        df.iloc[df[col] >= thresh[1], list(df.columns).index(col)] = np.nan\n",
    "        print(f\"added {df[col].isna().sum() - nan_before} nans in {col}\")\n",
    "\n",
    "print(df['temperature_sensors'].isna().sum())\n",
    "del_hard_thresholds_env(df)\n",
    "print(df['temperature_sensors'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## values with std. dev. 'nan' or zero\n",
    "If the standard deviation is 'nan', there was no or only one observation. If the standard deviation is zero, there was no fluctuation in the measured value, what can be assumed to be a measurement error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete values for the defined columns if the standard deviation is zero or 'nan'\n",
    "cols_env = [\n",
    "    'temperature_sensors',\n",
    "    'humidity_sensors',\n",
    "    'pressure_sensors',\n",
    "]\n",
    "\n",
    "def del_std_nan_env(df, cols=cols_env):\n",
    "    for col in cols:\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df.loc[df[col.split('_')[0]+'_std']==0, col] = np.nan    \n",
    "        df.loc[df[col.split('_')[0]+'_std']==np.nan, col] = np.nan    \n",
    "        print(f\"added {df[col].isna().sum() - nan_before} nans in {col}\")\n",
    "\n",
    "print(df['temperature_sensors'].isna().sum())\n",
    "del_std_nan_env(df)\n",
    "print(df['temperature_sensors'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dynamic thresholds based on quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quantiles as threshold\n",
    "thresh = {\n",
    "    'temperature': (.01, .85),\n",
    "    'humidity': (.05, .95),\n",
    "    'pressure': (.05, .95),\n",
    "}\n",
    "\n",
    "\n",
    "def del_dynamic_threshold_env(df, thresh=thresh):\n",
    "# make a dataframe containing median, upper and lower threshold defined by the quantiles above\n",
    "    df_thresholds = df.groupby(['city', 'timestamp']).agg(\n",
    "        temp_median = pd.NamedAgg(column='temperature_sensors', aggfunc='median'), \n",
    "        temp_lower = pd.NamedAgg(column='temperature_sensors', aggfunc=lambda x: x.quantile(q=thresh['temperature'][0])),\n",
    "        temp_upper = pd.NamedAgg(column='temperature_sensors', aggfunc=lambda x: x.quantile(q=thresh['temperature'][1])),\n",
    "        hum_median = pd.NamedAgg(column='humidity_sensors', aggfunc='median'), \n",
    "        hum_lower = pd.NamedAgg(column='humidity_sensors', aggfunc=lambda x: x.quantile(q=thresh['humidity'][0])),\n",
    "        hum_upper = pd.NamedAgg(column='humidity_sensors', aggfunc=lambda x: x.quantile(q=thresh['humidity'][1])),\n",
    "        pres_median = pd.NamedAgg(column='pressure_sensors', aggfunc='median'), \n",
    "        pres_lower = pd.NamedAgg(column='pressure_sensors', aggfunc=lambda x: x.quantile(q=thresh['pressure'][0])),\n",
    "        pres_upper = pd.NamedAgg(column='pressure_sensors', aggfunc=lambda x: x.quantile(q=thresh['pressure'][1])),\n",
    "    ).reset_index()\n",
    "\n",
    "    # merge the thresholds with the sc dataframe\n",
    "    df = df.merge(df_thresholds, how='left', on=['city', 'timestamp'])\n",
    "\n",
    "    # replace values below lower threshold and above upper threshold with 'nan'\n",
    "    for col, thresholds in {\n",
    "        'temperature_sensors': ['temp_lower', 'temp_upper'],\n",
    "        'humidity_sensors': ['hum_lower', 'hum_upper'],\n",
    "        'pressure_sensors': ['pres_lower','pres_upper'],\n",
    "    }.items():\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df.loc[(df[col] < df[thresholds[0]]) | (df[col] > df[thresholds[1]]), col] = np.nan\n",
    "        print(f\"{df[col].isna().sum() - nan_before} nans added in {col}\")\n",
    "\n",
    "    # drop columns used for dynamic thresholding\n",
    "    df.drop([col for col in df_thresholds.columns if not col in no_data_cols], axis=1, inplace=True)\n",
    "\n",
    "print(df['temperature_sensors'].isna().sum())\n",
    "del_dynamic_threshold_env(df)\n",
    "print(df['temperature_sensors'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_thresholds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace values below lower threshold and above upper threshold with 'nan'\n",
    "# for col, thresholds in {\n",
    "#     'temperature_sensors': ['temp_lower', 'temp_upper'],\n",
    "#     'humidity_sensors': ['hum_lower', 'hum_upper'],\n",
    "#     'pressure_sensors': ['pres_lower','pres_upper'],\n",
    "# }.items():\n",
    "#     nan_before = df[col].isna().sum()\n",
    "#     df.loc[(df[col] < df[thresholds[0]]) | (df[col] > df[thresholds[1]]), col] = np.nan\n",
    "#     print(f\"{df[col].isna().sum() - nan_before} nans added in {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop columns used for dynamic thresholding\n",
    "# df.drop([col for col in df_thresholds.columns if not col in no_data_cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of cleaned data and comparison with dwd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sc_vs_dwd(city, columns=1, reduction=1):\n",
    "    # Plot dwd and sc data \n",
    "    # define size of subplot\n",
    "    rows = int(np.ceil(3 / columns))\n",
    "\n",
    "    fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "    plt.suptitle(f\"Comparison sensor data vs. dwd in {city}\", fontsize=20) # title of plot\n",
    "    fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "    plt.subplots_adjust(hspace = .2, wspace = .2, top = .95) # adjusts the space between the single subplots\n",
    "\n",
    "    # Plot humidity from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['humidity_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='humidity_sensors', ax=ax[0], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['humidity_dwd'].notna()) & (df['city']== city)], x='timestamp', y='humidity_dwd', color='red', alpha=.5, ax=ax[0], label='Deutscher Wetterdienst')\n",
    "    ax[0].set_ylabel('Relative Humidity in %')\n",
    "\n",
    "    # Plot pressure from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['pressure_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='pressure_sensors', ax=ax[1], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['pressure_dwd'].notna()) & (df['city']== city)], x='timestamp', y='pressure_dwd', color='red', alpha=.5, ax=ax[1], label='Deutscher Wetterdienst')\n",
    "    ax[1].set_ylabel('Pressure in hPa')\n",
    "\n",
    "    # Plot temperature from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['temperature_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='temperature_sensors', ax=ax[2], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['temperature_dwd'].notna()) & (df['city']== city)], x='timestamp', y='temperature_dwd', color='red', alpha=.5, ax=ax[2], label='Deutscher Wetterdienst')\n",
    "    ax[2].set_ylabel('Temperature in Â°C')\n",
    "\n",
    "    xlim_left = df['timestamp'].min()\n",
    "    xlim_right = df['timestamp'].max()\n",
    "\n",
    "    # capitalize axis titles and add legend\n",
    "    for i in range(3):\n",
    "        ax[i].legend(loc='lower right')\n",
    "        ax[i].set_xlabel(ax[i].get_xlabel().capitalize())\n",
    "        ax[i].set_xlim(xlim_left, xlim_right)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot comparison of data from both sources for Frankfurt\n",
    "# plot_sc_vs_dwd('Frankfurt')\n",
    "# plt.savefig(\"../figures/EDA_sc_vs_dwd_Frankfurt.png\", bbox_inches='tight')\n",
    "# plt.close()\n",
    "# ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDA_sc_vs_dwd_Frankfurt.png](../figures/EDA_sc_vs_dwd_Frankfurt.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot comparison of data from both sources for Bremen\n",
    "# plot_sc_vs_dwd('Bremen')\n",
    "# plt.savefig(\"../figures/EDA_sc_vs_dwd_Bremen.png\", bbox_inches='tight')\n",
    "# plt.close()\n",
    "# ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDA_sc_vs_dwd_Bremen.png](../figures/EDA_sc_vs_dwd_Bremen.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the distribution of measured temperatures in one day\n",
    "sns.histplot(data=df[(df['timestamp'] > '2020-07-01') & (df['timestamp'] < '2020-07-15')], x='temperature_sensors', bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of single locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by location_id and calculate the total number of hours with measurements, date of the first and of the last measurement\n",
    "location_grouped = df[(df['PM10'].notna()) & (df['PM2p5'].notna())][['location_id', 'timestamp']].\\\n",
    "    groupby(['location_id']).\\\n",
    "        agg(\n",
    "                hours = pd.NamedAgg(column='timestamp', aggfunc='count'), \n",
    "                date_min = pd.NamedAgg(column='timestamp', aggfunc='min'),\n",
    "                date_max = pd.NamedAgg(column='timestamp', aggfunc='max')\n",
    "            ).\\\n",
    "            reset_index().\\\n",
    "                sort_values('hours', ascending=False)\n",
    "\n",
    "location_grouped['date_min'] = pd.to_datetime(location_grouped['date_min'])\n",
    "location_grouped['date_max'] = pd.to_datetime(location_grouped['date_max'])\n",
    "location_grouped['period_length'] = location_grouped['date_max'] - location_grouped['date_min'] + pd.Timedelta(days=1)\n",
    "location_grouped['hours_per_day'] = location_grouped['hours'] / location_grouped['period_length'].dt.days\n",
    "location_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of hours that were measured at each location\n",
    "plt.figure(figsize=(25, 10))\n",
    "g = sns.barplot(data=location_grouped, x='location_id', y='hours', order=location_grouped.sort_values('hours', ascending=False)['location_id'])\n",
    "g.set_xlabel(g.get_xlabel().capitalize().replace('_', ' '))\n",
    "g.set_ylabel(g.get_ylabel().capitalize())\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of hours per day measured per location\n",
    "plt.figure(figsize=(25, 10))\n",
    "g = sns.barplot(data=location_grouped.sort_values('hours_per_day', ascending=False), x='location_id', y='hours_per_day', order=location_grouped.sort_values('hours_per_day', ascending=False)['location_id'])\n",
    "g.set_xlabel(g.get_xlabel().capitalize().replace('_', ' '))\n",
    "g.set_ylabel(g.get_ylabel().capitalize().replace('_', ' '))\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of locations: {location_grouped.shape[0]}\")\n",
    "print('Locations with the least hours of measurement:')\n",
    "location_grouped.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_grouped[['hours', 'hours_per_day']].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some sensor locations which delivered data only for few hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_PM(df):\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 20))\n",
    "    plt.suptitle(\"Sensors per City\", fontsize=20) # title of plot\n",
    "    fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "    plt.subplots_adjust(hspace = .5, wspace = .2, top = .9) # adjusts the space between the single subplots\n",
    "\n",
    "    # get ids match them with the cities\n",
    "    labels_frankfurt = set(df.query(\"city=='Frankfurt'\")['location_id'])\n",
    "    labels_bremen = set(df.query(\"city=='Bremen'\")['location_id'])\n",
    "\n",
    "    # plot PM10 data of Frankfurt\n",
    "    sns.lineplot(data=df[df['city']=='Frankfurt'][::10], x='timestamp', y='PM10', hue='location_id', ax=ax1, legend=False)\n",
    "    ax1.legend(labels=labels_frankfurt) # assign a unique color to every id\n",
    "    ax1.set_title('Frankfurt - PM10', fontsize = 15) # set title and font size\n",
    "    ax1.legend([], [], frameon=False) # hide legend\n",
    "\n",
    "    # plot PM2.5 data for Frankfurt\n",
    "    sns.lineplot(data=df[df['city']=='Frankfurt'][::10], x='timestamp', y='PM2p5', hue='location_id', ax=ax2, legend=False)\n",
    "    ax2.legend(labels=labels_frankfurt)\n",
    "    ax2.set_title('Frankfurt - PM2.5', fontsize = 15)\n",
    "    ax2.legend([], [], frameon=False)\n",
    "\n",
    "    # plot PM10 data for Bremen\n",
    "    sns.lineplot(data=df[df['city']=='Bremen'][::10], x='timestamp', y='PM10', hue='location_id', ax=ax3, legend=False)\n",
    "    ax3.legend(labels=labels_bremen)\n",
    "    ax3.set_title('Bremen - PM10', fontsize = 15)\n",
    "    ax3.legend([], [], frameon=False)\n",
    "\n",
    "    # plot PM2.5 data for Bremen\n",
    "    sns.lineplot(data=df[df['city']=='Bremen'][::10], x='timestamp', y='PM2p5', hue='location_id', ax=ax4, legend=False)\n",
    "    ax4.legend(labels=labels_bremen)\n",
    "    ax4.set_title('Bremen - PM2.5', fontsize = 15)\n",
    "    ax4.legend([], [], frameon=False)\n",
    "\n",
    "plot_all_PM(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example location (location_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get location_id's occuring in Frankfurt\n",
    "ids_frankfurt = df.query(\"city=='Frankfurt'\")['location_id'].unique()\n",
    "\n",
    "# plot PM10, PM2.5 and humidity of one location\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='PM10', color='b', alpha=.5)\n",
    "sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='PM2p5', color=\"r\", alpha=.5, ax=ax)\n",
    "ax2 = ax.twinx() # add second y-axis\n",
    "sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='humidity_sensors', color=\"g\", alpha=.5, ax=ax2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap for one single location\n",
    "sns.heatmap(df[df['location_id']==ids_frankfurt[0]][sc_cols_wo_std].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe containing timestamps of one year with resolution of one hour\n",
    "one_year_full = pd.DataFrame()\n",
    "one_year_full['timestamp'] = pd.date_range(\"2021-03-01\", \"2022-02-28 23:00:00\", freq=\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add observations of one location to that dataframe\n",
    "one_year_full_2 = pd.merge(one_year_full, df[df['location_id']==ids_frankfurt[0]], how='left', on='timestamp')\n",
    "print(f\"{one_year_full_2['PM10'].isna().sum()} missing values in PM10\")\n",
    "print(f\"{one_year_full_2['PM2p5'].isna().sum()} missing values in PM2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of observations where PM10 value is 'NaN'\n",
    "missing_index = one_year_full_2.index[one_year_full_2['PM10'].isna()].tolist()\n",
    "\n",
    "missing_periods = [] # list for periods of missing values\n",
    "i = 0 # index for loop\n",
    "start = None # start of a period\n",
    "previous = None # index of the previous loop\n",
    "\n",
    "\n",
    "while i < len(missing_index):\n",
    "    # if start is None, it is the first loop\n",
    "    if start == None:\n",
    "        start = previous = missing_index[i]\n",
    "        i += 1\n",
    "        continue\n",
    "    # if the current index is the previous index + 1, we are still moving within a closed period\n",
    "    if missing_index[i] == previous+1:\n",
    "        previous = missing_index[i]\n",
    "        i += 1\n",
    "        continue\n",
    "    # else one period is over and another one is starting\n",
    "    else:\n",
    "        # add the closed period to the list of missing periods\n",
    "        missing_periods.append(\n",
    "            (one_year_full_2['timestamp'][start], \n",
    "            one_year_full_2['timestamp'][previous], \n",
    "            one_year_full_2['timestamp'][previous] - one_year_full_2['timestamp'][start] + pd.Timedelta(1, 'hour'))\n",
    "        )\n",
    "        start = previous = missing_index[i]\n",
    "        i += 1\n",
    "# add the last period to the list\n",
    "missing_periods.append(\n",
    "    (one_year_full_2['timestamp'][start], \n",
    "    one_year_full_2['timestamp'][previous], \n",
    "    one_year_full_2['timestamp'][previous]  - one_year_full_2['timestamp'][start] + pd.Timedelta(1, 'hour'))\n",
    ")\n",
    "\n",
    "# print the periods of missing PM10 values and their duration\n",
    "p = 0\n",
    "for start, end, duration in missing_periods:\n",
    "    p += 1\n",
    "    print(f\"Period of missing values #{p}:\\n\\tstart: {start}\\n\\tend: {end}\\n\\tduration: {duration}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dynamic thresholds for PM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL: Calculate a dynamic median per hour for all sensors in a city. If a value is for example three times the median it is estimated to be an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pm(df: pd.DataFrame, cols: list=['PM10', 'PM2p5'], factor: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"deletes outliers for the given columns and considerung their timestamps and cities which are larger than factor times the median\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe\n",
    "        cols (list): columns to clean\n",
    "        factor (int, optional): factor that is used to calculate the threshold for keeping or deleting data. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    for col in df.columns:\n",
    "        if 'threshold' in col:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # define a list for saving the thresholds\n",
    "    thresholds = []\n",
    "\n",
    "    # for each city in the dataframe make a dataframe with timestamps\n",
    "    for city in df['city'].unique():\n",
    "        df_cur = df[df['city'] == city]\n",
    "        df_threshold = pd.DataFrame(\n",
    "            data={\n",
    "                'timestamp': df_cur['timestamp'].unique(), \n",
    "                'city': city\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # for each timestamp calculate the median and threshold (factor * median)\n",
    "        for col in cols:\n",
    "            df_threshold[col+'_median'] = df_threshold.apply(lambda x: df_cur[(df_cur['timestamp'] == x['timestamp'])][col].median(), axis=1)\n",
    "            df_threshold[col+'_threshold'] = factor * df_threshold[col+'_median']\n",
    "        thresholds.append(df_threshold)\n",
    "\n",
    "    # concatenate all thresholds\n",
    "    df_thresholds = pd.DataFrame()\n",
    "    for df_threshold in thresholds:\n",
    "        df_thresholds = pd.concat([df_thresholds, df_threshold])\n",
    "    \n",
    "    # merge thresholds with original dataframe on timestamp and city \n",
    "    df = df.merge(df_thresholds, how='left', on=['timestamp', 'city'])\n",
    "    \n",
    "    # delete values if they are above the threshold and print number of deleted values\n",
    "    for col in cols:\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df[col] = df.apply(lambda x: x[col] if x[col] <= x[col+'_threshold'] else np.nan, axis=1)\n",
    "        print(f\"{df[col].isna().sum() - nan_before} NaNs added in {col}\")\n",
    "\n",
    "    # for col in cols:\n",
    "    #     df.drop([col+'_threshold'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = clean_pm(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_PM(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PM_data_per_location(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): Dataframe containing data of PM sensors\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing one dataframe per city and PM sensor\n",
    "    \"\"\"\n",
    "    # make dataframe containing the timestamps\n",
    "    df_missing_values_bremen_pm10 = pd.DataFrame(\n",
    "        data={\n",
    "            'timestamp': df['timestamp'].unique(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # copy that dataframe for every combination of PM sensor and city\n",
    "    df_missing_values_bremen_pm2p5 = df_missing_values_bremen_pm10.copy()\n",
    "    df_missing_values_frankfurt_pm10 = df_missing_values_bremen_pm10.copy()\n",
    "    df_missing_values_frankfurt_pm2p5 = df_missing_values_bremen_pm10.copy()\n",
    "\n",
    "    # add sensor data for every location in Bremen\n",
    "    for location in df.loc[df['city'] == 'Bremen', 'location_id'].unique():\n",
    "        df_missing_values_bremen_pm10 = pd.merge(df_missing_values_bremen_pm10, df.loc[df['location_id']==location, ['timestamp','PM10']], on='timestamp')\n",
    "        df_missing_values_bremen_pm10.rename(columns={'PM10': location}, inplace=True) # rename the new column using the location_id\n",
    "        df_missing_values_bremen_pm10.set_index('timestamp', inplace=True) # use timestamps as index\n",
    "\n",
    "        df_missing_values_bremen_pm2p5 = pd.merge(df_missing_values_bremen_pm2p5, df.loc[df['location_id']==location, ['timestamp','PM2p5']], on='timestamp')\n",
    "        df_missing_values_bremen_pm2p5.rename(columns={'PM2p5': location}, inplace=True)\n",
    "        df_missing_values_bremen_pm2p5.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # do the same for Frankfurt\n",
    "    for location in df.loc[df['city'] == 'Frankfurt', 'location_id'].unique():\n",
    "        df_missing_values_frankfurt_pm10 = pd.merge(df_missing_values_frankfurt_pm10, df.loc[df['location_id']==location, ['timestamp','PM10']], on='timestamp')\n",
    "        df_missing_values_frankfurt_pm10.rename(columns={'PM10': location}, inplace=True)\n",
    "        df_missing_values_frankfurt_pm10.set_index('timestamp', inplace=True)\n",
    "\n",
    "        df_missing_values_frankfurt_pm2p5 = pd.merge(df_missing_values_frankfurt_pm2p5, df.loc[df['location_id']==location, ['timestamp','PM2p5']], on='timestamp')\n",
    "        df_missing_values_frankfurt_pm2p5.rename(columns={'PM2p5': location}, inplace=True)\n",
    "        df_missing_values_frankfurt_pm2p5.set_index('timestamp', inplace=True)\n",
    "    return  df_missing_values_bremen_pm10, df_missing_values_bremen_pm2p5, df_missing_values_frankfurt_pm10, df_missing_values_frankfurt_pm2p5\n",
    "\n",
    "\n",
    "df_missing_values_bremen_pm10, df_missing_values_bremen_pm2p5, df_missing_values_frankfurt_pm10, df_missing_values_frankfurt_pm2p5 = get_PM_data_per_location(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 15))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 15))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 15))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop sensors with only few data in the past year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../data/df_backup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sns.set_theme()\n",
    "# plt.rcParams.update({'figure.facecolor':'white'})\n",
    "\n",
    "# df = pd.read_csv(\"../data/df_backup.csv\", index_col=0)\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_share_of_missing_values(df: pd.DataFrame, start_time: str):\n",
    "    # Get the total number of observations possible in the past year\n",
    "    observations_of_interest = df[(df['location_id'] == df['location_id'].unique()[0]) & (df['timestamp'] >= pd.to_datetime(start_time))].shape[0]\n",
    "\n",
    "    # make a dataframe to store missing values per location\n",
    "    missing_values = pd.DataFrame(columns=['location_id', 'city', 'PM10_missing', 'PM2p5_missing'])\n",
    "\n",
    "    # get missing values for every location\n",
    "    for location in df['location_id'].unique():\n",
    "        # filter for location\n",
    "        df_cur = df[(df['location_id'] == location) & (df['timestamp'] >= pd.to_datetime('2021-01-01'))][['city', 'PM10', 'PM2p5']]\n",
    "        \n",
    "        # create a new entry in the dataframe containing location_id, city and share of missing values\n",
    "        new_entry = {\n",
    "            'location_id': int(location),\n",
    "            'city': df_cur['city'].iloc[0],\n",
    "            'PM10_missing': df_cur['PM10'].isna().sum() / observations_of_interest,\n",
    "            'PM2p5_missing': df_cur['PM2p5'].isna().sum() / observations_of_interest,\n",
    "        }\n",
    "        missing_values = missing_values.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # cast location_id to int\n",
    "    missing_values['location_id'] = missing_values['location_id'].astype(int) \n",
    "    return missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_values = get_share_of_missing_values(df, '2021-01-01')\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,1,figsize=(20,15))\n",
    "plt.suptitle(\"Missing values per city and sensor\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .4, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "i=0\n",
    "# plot share of missing values for every city and PM sensor\n",
    "for city in missing_values['city'].unique():\n",
    "    for col in ['PM10_missing', 'PM2p5_missing']:\n",
    "        sns.barplot(\n",
    "            data=missing_values[missing_values['city']==city],\n",
    "            x='location_id',\n",
    "            y=col,\n",
    "            order=missing_values[missing_values['city']==city].sort_values(col, ascending=False)['location_id'], # sort by missing values\n",
    "            ax=ax[i]\n",
    "        )\n",
    "        ax[i].tick_params(labelrotation=90) # rotate x tick labels\n",
    "        ax[i].set_title(city + ' - ' + col.split('_')[0].replace('p', '.')) # set a title (City - Sensor)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the IDs of good sensors having less than 25 % missing values in PM2.5\n",
    "good_sensors = missing_values.query(\"PM2p5_missing < 0.25\")['location_id']\n",
    "good_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data of those good sensors\n",
    "def use_good_sensors_only(df, good_sensors=good_sensors):\n",
    "    df_good_sensors = df[df['location_id'].\\\n",
    "        isin(good_sensors)].\\\n",
    "            drop([col for col in df.columns if ('median' in col or 'threshold' in col)], axis=1)\n",
    "    return df_good_sensors\n",
    "\n",
    "df_good_sensors = use_good_sensors_only(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df_good_sensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test data\n",
    "df_test = pd.read_csv(\"../data/processed_sensor_dwd_test.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign location IDs according to coordinates\n",
    "df_test['location_id'] = df_test.apply(\n",
    "    lambda x: location_id_assignment.\\\n",
    "        loc[(location_id_assignment['lat'] == x['lat']) & (location_id_assignment['lon'] == x['lon']), 'location_id'].\\\n",
    "            iloc[0], \n",
    "            axis=1\n",
    ")\n",
    "df_test['location_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime\n",
    "df_test['timestamp'] = pd.to_datetime(df_test['timestamp'])\n",
    "\n",
    "# sort columns\n",
    "df_test = df_test.reindex(columns=no_data_cols + sc_cols + dwd_cols)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers of environmental parameters by different mechanisms\n",
    "print(\"hard thresholds\")\n",
    "del_hard_thresholds_env(df_test)\n",
    "\n",
    "print(\"constant values\")\n",
    "del_std_nan_env(df_test)\n",
    "\n",
    "print(\"dnyamic thersholds\")\n",
    "del_dynamic_threshold_env(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all PM data\n",
    "plot_all_PM(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get missing values of PM data per sensor\n",
    "df_missing_values_bremen_pm10_test, \\\n",
    "df_missing_values_bremen_pm2p5_test, \\\n",
    "df_missing_values_frankfurt_pm10_test, \\\n",
    "df_missing_values_frankfurt_pm2p5_test = get_PM_data_per_location(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean PM data\n",
    "df_test = clean_pm(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data of sensors marked as good\n",
    "df_good_sensors_test = use_good_sensors_only(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.shape)\n",
    "print(df_good_sensors_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of 'good sensors' should be identical to locations in test dataframe\n",
    "print(len(good_sensors))\n",
    "df_good_sensors_test['location_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all PM data per location\n",
    "plot_all_PM(df_good_sensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get missing values per sensor\n",
    "df_missing_values_bremen_pm10_test, \\\n",
    "df_missing_values_bremen_pm2p5_test, \\\n",
    "df_missing_values_frankfurt_pm10_test, \\\n",
    "df_missing_values_frankfurt_pm2p5_test = get_PM_data_per_location(df_good_sensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get share of missing values in the cleaned test dataframe\n",
    "missing_values_test = get_share_of_missing_values(df_good_sensors_test, \"2021-01-01\")\n",
    "\n",
    "# make a series of good sensors in test data (less than 75 % missing in PM2.5)\n",
    "good_sensors_test = missing_values_test.query(\"PM2p5_missing < 0.25\")['location_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bad sensors in test dataframe (more than 75 % of PM2.5 data missing)\n",
    "bad_sensors = []\n",
    "for location in list(good_sensors):\n",
    "    if location not in list(good_sensors_test):\n",
    "        bad_sensors.append(location)\n",
    "\n",
    "print(len(bad_sensors))\n",
    "bad_sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update dataframes using only 'good sensors' and save cleaned train and test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update train dataframe according to good sensors in test data\n",
    "df_good_sensors = use_good_sensors_only(df, good_sensors_test)\n",
    "\n",
    "# save train data for good sensors\n",
    "df_good_sensors.to_csv(\"../data/cleaned_sensors_dwd_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test dataframe containing only good sensors\n",
    "df_good_sensors_test = use_good_sensors_only(df_test, good_sensors_test)\n",
    "\n",
    "# save test data\n",
    "df_good_sensors_test.to_csv(\"../data/cleaned_sensors_dwd_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last check of missing data in the final dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_values_bremen_pm10_test, \\\n",
    "df_missing_values_bremen_pm2p5_test, \\\n",
    "df_missing_values_frankfurt_pm10_test, \\\n",
    "df_missing_values_frankfurt_pm2p5_test = get_PM_data_per_location(df_good_sensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_values_bremen_pm10, \\\n",
    "df_missing_values_bremen_pm2p5, \\\n",
    "df_missing_values_frankfurt_pm10, \\\n",
    "df_missing_values_frankfurt_pm2p5 = get_PM_data_per_location(df_good_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good_sensors['location_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PM2.5 concentration of the train time frame and add Corona lockdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://de.wikipedia.org/wiki/COVID-19-Pandemie_in_Deutschland#Reaktionen_und_MaÃnahmen_der_Politik\n",
    "# corona lockdowns (start date, end date, 'strength' (used for transparency))\n",
    "corona_lockdowns = [\n",
    "    (\"2020-03-22\", \"2020-05-06\", 3),\n",
    "    (\"2020-11-02\", \"2020-12-16\", 1.5), # 2020-11-02: lockdown light, 2020-12-16:lockdown\n",
    "    (\"2020-12-16\", \"2021-03-03\", 3), # 2020-11-02: lockdown light, 2020-12-16:lockdown\n",
    "    (\"2021-04-23\", \"2021-06-03\", 3), # Bundesnotbremse\n",
    "]\n",
    "\n",
    "# # locations to save the image\n",
    "# save_locations = [\n",
    "#     125,\n",
    "#     12,\n",
    "#     11,\n",
    "#     159,\n",
    "#     84,\n",
    "#     111,\n",
    "# ]\n",
    "\n",
    "# set the upper limits of the y axis for every location\n",
    "y_limits = {\n",
    "    125: 105,\n",
    "    12: 80,\n",
    "    11: 70,\n",
    "    159: 75,\n",
    "    84: 75,\n",
    "    111: 80,\n",
    "}\n",
    "\n",
    "# set seaborn theme and set context to talk to increase the size of labels\n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "\n",
    "    for location, y_limit in y_limits.items(): # for location in save_locations: # list(df_good_sensors['location_id'].unique())[:]: # \n",
    "        plt.figure(figsize=(25,5))\n",
    "        g = sns.lineplot(data=df_good_sensors[df_good_sensors['location_id']==location], x='timestamp', y='PM2p5', label='PM2.5 conc.')\n",
    "\n",
    "        # g.set_title(str(df_good_sensors.loc[df_good_sensors['location_id']==location, 'city'].iloc[0]) + ' (ID: ' + str(location) + ')')\n",
    "        ax = g.axes # get axes of g\n",
    "        g.set(\n",
    "            xlabel='',\n",
    "            ylabel=g.get_ylabel().replace('p', '.') + ' in Âµg/m$^3$',\n",
    "            facecolor='#EEEEEE',\n",
    "            xlim=(pd.to_datetime('2020-01-01') - pd.Timedelta(10, 'D'), pd.to_datetime('2021-12-31') + pd.Timedelta(10, 'D')),\n",
    "            # ylim=(-5, 105),\n",
    "            ylim=(-5, y_limit) # set y_lim according to the values specified above\n",
    "        )\n",
    "\n",
    "        # add corona lockdowns as red boxes\n",
    "        for lockdown in corona_lockdowns:\n",
    "            g.axvspan(pd.to_datetime(lockdown[0]), pd.to_datetime(lockdown[1]), alpha=lockdown[2]/10, color='red', label='Corona Lockdown')\n",
    "        \n",
    "        # add a legend and show the first two entries (PM and lockdown)\n",
    "        handles, labels = g.get_legend_handles_labels()\n",
    "        g.legend(handles=handles[:2], labels=labels[:2], frameon=False)\n",
    "\n",
    "        # save figure\n",
    "        # if location in save_locations:\n",
    "        plt.savefig('../figures/EDA_PM2p5_lockdowns_' + str(location) + '.png', transparent=True, bbox_inches='tight')\n",
    "        # plt.savefig('../figures/EDA_PM2p5_lockdowns_' + str(location) + '.png', facecolor=g.get_facecolor(), bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://de.wikipedia.org/wiki/COVID-19-Pandemie_in_Deutschland#Reaktionen_und_MaÃnahmen_der_Politik\n",
    "corona_lockdowns = [\n",
    "    (\"2020-03-22\", \"2020-05-06\", 3),\n",
    "    (\"2020-11-02\", \"2020-12-16\", 1.5), # 2020-11-02: lockdown light, 2020-12-16:lockdown\n",
    "    (\"2020-12-16\", \"2021-03-03\", 3), # 2020-11-02: lockdown light, 2020-12-16:lockdown\n",
    "    (\"2021-04-23\", \"2021-06-03\", 3), # Bundesnotbremse\n",
    "]\n",
    "\n",
    "# define the weeks to plot\n",
    "weeks = pd.date_range(pd.to_datetime('2021-01-01'), pd.to_datetime('2021-12-31'), freq='W')\n",
    "\n",
    "# location to plot\n",
    "location=125 # good: 80\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    for i, week in enumerate(weeks[:-1]):\n",
    "        plt.figure(figsize=(25,5))\n",
    "\n",
    "        # plot PM2.5 values for each week\n",
    "        g = sns.lineplot(data=df_good_sensors[df_good_sensors['location_id']==location], x='timestamp', y='PM2p5', label='PM2.5 conc.', legend=False)\n",
    "\n",
    "        # use city and location_id as title\n",
    "        g.set_title(str(df_good_sensors.loc[df_good_sensors['location_id']==location, 'city'].iloc[0]) + ' (ID: ' + str(location) + ')')\n",
    "        g.set(\n",
    "            xlabel='',\n",
    "            ylabel=g.get_ylabel().replace('p', '.') + ' in Âµg/m$^3$',\n",
    "            # facecolor='#EEEEEE',\n",
    "            xlim=(weeks[i], weeks[i+1]), # set x_lim to show only one week\n",
    "            ylim=(-5, 100),\n",
    "        )\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe grouped by time and id and calculate mean for that time frame\n",
    "df_good_sensors['weekday'] = df_good_sensors['timestamp'].dt.weekday\n",
    "df_good_sensors['day_name'] = df_good_sensors['timestamp'].dt.day_name()\n",
    "df_good_sensors['week'] = df_good_sensors['timestamp'].dt.week\n",
    "df_good_sensors['hour'] = df_good_sensors['timestamp'].dt.hour\n",
    "df_good_sensors_weekdays = df_good_sensors.groupby(['location_id', 'city', 'week', 'weekday', 'day_name', 'hour']).mean()[['PM10', 'PM2p5']].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change list in the first code line to define which locations to show\n",
    "for location in [125]: # list(df_good_sensors_weekdays['location_id'].unique())[:5]: # \n",
    "    fig, ax = plt.subplots(1, 7, figsize=(30,10))\n",
    "    fig.suptitle('ID: ' + str(location)) # add ID as super title\n",
    "\n",
    "    # Plot PM2.5 per weekday and ID\n",
    "    for day in list(df_good_sensors_weekdays['weekday'].unique())[:]: \n",
    "        data = df_good_sensors_weekdays[(df_good_sensors_weekdays['weekday']==day) & (df_good_sensors_weekdays['location_id']==location)]\n",
    "        g=sns.lineplot(data=data, x='hour', y='PM2p5', ax=ax[day])\n",
    "        g.set_ylim(0, 26)\n",
    "        g.set_title(data['day_name'].iloc[0])\n",
    "\n",
    "    # Use the y label only for the first subplot\n",
    "    ax[0].set_ylabel(g.get_ylabel().replace('p', '.') + ' in Âµg/m$^3$',)\n",
    "    for a in ax[1:]:\n",
    "        a.set_ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 7, figsize=(30,10))\n",
    "fig.suptitle('Average over all locations and all weeks') # add ID as super title\n",
    "\n",
    "# Plot PM2.5 per weekday and ID\n",
    "for day in list(df_good_sensors_weekdays['weekday'].unique())[:]: \n",
    "    data = df_good_sensors_weekdays[(df_good_sensors_weekdays['weekday']==day)]\n",
    "    g=sns.lineplot(data=data, x='hour', y='PM2p5', ax=ax[day])\n",
    "    g.set_ylim(0, 10)\n",
    "    g.set_xlim(0, 23)\n",
    "    g.set_title(data['day_name'].iloc[0])\n",
    "\n",
    "# Use the y label only for the first subplot\n",
    "ax[0].set_ylabel(g.get_ylabel().replace('p', '.') + ' in Âµg/m$^3$',)\n",
    "for a in ax[1:]:\n",
    "    a.set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 125\n",
    "# Plot average PM2.5 for one week\n",
    "plt.figure(figsize=(20,8))\n",
    "g=sns.lineplot(data=df_good_sensors_weekdays[df_good_sensors_weekdays['location_id']==location], x='weekday', y='PM2p5')\n",
    "g.set_ylim(0,g.get_ylim()[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of PM2p5 with features (regressors for Prophet) for final train data\n",
    "### First look at one location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data if notebook not run so far\n",
    "df_good_sensors = pd.read_csv(\"../data/cleaned_sensors_dwd_train.csv\")\n",
    "# convert timestamp to datetime\n",
    "df_good_sensors['timestamp'] = pd.to_datetime(df_good_sensors['timestamp'])\n",
    "df_good_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for most complete location\n",
    "df_good_sensors.groupby(['location_id'], dropna=False).PM2p5.count().sort_values()\n",
    "location = 98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show correlation map for one location\n",
    "columns_plot = ['PM10', 'PM2p5', 'humidity_dwd', 'precip', 'pressure_dwd', 'pressure_sealevel', 'temperature_dwd', 'wind_direction', 'wind_speed']\n",
    "corr_mtrx = df_good_sensors.query(f'location_id == {location}')[columns_plot].corr()\n",
    "plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_mtrx, dtype=bool))\n",
    "sns.heatmap(corr_mtrx, annot=True, cmap=\"YlGnBu_r\", mask=mask, vmax=1, vmin=-1,fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PM2p5 correlates for this location most with humidity, temperature and wind speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second look at all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be considered for correlation\n",
    "columns_corr = ['PM10', 'PM2p5', 'humidity_dwd', 'pressure_dwd', 'precip', 'wind_direction', 'wind_speed', 'temperature_dwd']\n",
    "# created sorted list of location_ids\n",
    "location_list = np.sort(df_good_sensors.location_id.unique())\n",
    "\n",
    "# create DataFrame for correlations of PM2p5 with given features = prophet regressors\n",
    "corr_mtrx = df_good_sensors.query(f'location_id == {location_list[0]}')[columns_corr].corr(method='pearson')\n",
    "df_PM2p5_correlations = pd.DataFrame(corr_mtrx.iloc[1,2:]) # correlation of PM2p5: iloc[1,2:], PM10: iloc[0,2:]\n",
    "\n",
    "# fill DataFrame\n",
    "for i in location_list:\n",
    "    corr_mtrx = df_good_sensors.query(f'location_id == {i}')[columns_corr].corr(method='pearson')\n",
    "    df_PM2p5_correlations[i] = corr_mtrx.iloc[1,2:]\n",
    "\n",
    "# drop double column and transpose\n",
    "df_PM2p5_correlations = df_PM2p5_correlations.drop('PM2p5', axis=1).T\n",
    "\n",
    "display(df_PM2p5_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "df_PM2p5_correlations.plot(figsize=(20,12))\n",
    "plt.title('                        Frankfurt                                                                          Bremen', fontsize=20)\n",
    "plt.xlabel('location_id', fontsize=20)\n",
    "plt.ylabel('correlation with PM2p5', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.ylim(-0.5,0.5)\n",
    "plt.xlim(0)\n",
    "plt.plot([124.5, 124.5], [-0.5, 0.5], linewidth=5, color='black')\n",
    "#plt.plot([0, 50], [0, 1], linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations with PM2p5\n",
    "> * Humidity, temperature and wind speed are the most important features for most locations.\n",
    "> * For Bremen temperature and wind speed seem a little less important.\n",
    "> * For Frankfurt pressure seems to play a roll in contrast to Bremen.\n",
    "> * Precipitation shows unexpectedly no correlation with PM2p5. This is maybe due to a time shift: When it's raining now the PM values will increase within some hours...\n",
    "> * Some locations show no correlation at all.\n",
    "\n",
    "Let's have a deeper look at those correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataframe with rounded values for histplot\n",
    "df_round = df_good_sensors.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.suptitle('25-75 % of feature values vs. PM2.5 for all locations')\n",
    "\n",
    "plt.subplot(4,1,1)\n",
    "sns.boxplot(data=df_round, y='humidity_dwd', x='PM2p5', hue='city', showfliers=False, whis=0)\n",
    "plt.xlim(0,30)\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "sns.boxplot(data=df_round, y='pressure_dwd', x='PM2p5', hue='city', showfliers=False, whis=0)\n",
    "plt.xlim(0,30)\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "sns.boxplot(data=df_round, y='wind_speed', x='PM2p5', hue='city', showfliers=False, whis=0)\n",
    "plt.xlim(0,30)\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "sns.boxplot(data=df_round, y='temperature_dwd', x='PM2p5', hue='city', showfliers=False, whis=0)\n",
    "plt.xlim(0,30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> same findings like above for looking at all locations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc means per city\n",
    "df_mean_per_city = df_good_sensors.groupby(['city'])['PM10', 'PM2p5', 'humidity_dwd', 'precip', 'pressure_dwd', 'temperature_dwd', 'wind_direction', 'wind_speed'].mean().reset_index()\n",
    "df_mean_per_city\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out which locations show hardly a correlation between PM2p5 and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for location_id\n",
    "df_PM2p5_correlations['location_id'] = df_PM2p5_correlations.index\n",
    "\n",
    "# add city column \n",
    "df_PM2p5_correlations['city'] = df_PM2p5_correlations['location_id'].apply(lambda x: \"Frankfurt\" if x < 125 else 'Bremen')\n",
    "df_PM2p5_correlations\n",
    "\n",
    "# calculate sum of correlation absolutes\n",
    "df_PM2p5_correlations['corr_sum_abs'] = df_PM2p5_correlations['humidity_dwd'].abs() + df_PM2p5_correlations['precip'].abs() + df_PM2p5_correlations['pressure_dwd'].abs() + df_PM2p5_correlations['temperature_dwd'].abs() + df_PM2p5_correlations['wind_direction'].abs() + df_PM2p5_correlations['wind_speed'].abs()\n",
    "\n",
    "display(df_PM2p5_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(data=df_PM2p5_correlations.sort_values('corr_sum_abs'), x='location_id', y='corr_sum_abs', hue='city', order=df_PM2p5_correlations.sort_values('corr_sum_abs')['location_id'])\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * corr_sum_abs could be a measure for the quality of our PM prediction. If corr_sum_abs is small, there are few correlations with regressors and no good prediction can be expected?\n",
    "> * Why are the PM values in Bremen less correlated to weather data? Is it a question of mean PM concentration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add averages of PM2p5 for comparison\n",
    "df_PM2p5_correlations['PM2p5_mean'] = df_good_sensors.groupby(['location_id']).mean()['PM2p5']\n",
    "df_PM2p5_correlations['PM2p5_median'] = df_good_sensors.groupby(['location_id']).median()['PM2p5']\n",
    "df_PM2p5_correlations['PM2p5_quantile_99'] = df_good_sensors.groupby(['location_id']).quantile(0.99)['PM2p5']\n",
    "df_PM2p5_correlations['PM2p5_max'] = df_good_sensors.groupby(['location_id']).max()['PM2p5']\n",
    "df_PM2p5_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(30,8))\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(data=df_PM2p5_correlations, x='PM2p5_median', y='corr_sum_abs', style='city', s=200)\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.scatterplot(data=df_PM2p5_correlations, x='PM2p5_quantile_99', y='corr_sum_abs', style='city', s=200)\n",
    "#plt.legend(['mean Frankfurt', 'mean Bremen', 'median Frankfurt', 'median Bremen'])\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.scatterplot(data=df_PM2p5_correlations, x='PM2p5_max', y='corr_sum_abs', style='city', s=200)\n",
    "#plt.legend(['mean Frankfurt', 'mean Bremen', 'median Frankfurt', 'median Bremen'])\n",
    "plt.xlim(0)\n",
    "plt.ylim(0);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Median (left): Locations with generally small PM2p5 values show few correlation with weather data. This is reasonable as wind or humidity will not reduce PM if it's not there.\n",
    "> * Overall there there is a stronger correlation for Frankfurt on weather data than for Bremen. Even though both cities cover a comparable wide range of PM values. This is mostly due to the missing correlation with air pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a deeper look at precipitation\n",
    "Is there really no correlation with PM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PM as a function of precipitation without and with time shift of - 1 hour\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2,1,1)\n",
    "sns.lineplot(x=df_good_sensors.query(f'precip > 0')['precip'], y=df_good_sensors.query('precip > 0')['PM10'].shift(-1))\n",
    "sns.lineplot(x=df_good_sensors.query(f'precip > 0')['precip'], y=df_good_sensors.query('precip > 0')['PM10'])\n",
    "plt.legend(['with shift -1', '', 'without shift', '', 'with shift +1'])\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.lineplot(x=df_good_sensors.query(f'precip > 0')['precip'], y=df_good_sensors.query('precip > 0')['PM2p5'].shift(-1))\n",
    "sns.lineplot(x=df_good_sensors.query(f'precip > 0')['precip'], y=df_good_sensors.query('precip > 0')['PM2p5'])\n",
    "plt.legend(['with shift -1', '', 'without shift', '', 'with shift +1'])\n",
    "plt.xlim(0)\n",
    "plt.ylim(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is actually no correlation between PM values and amount of precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with 1 for rain and 0 for no rain\n",
    "df_good_sensors['precip_bool'] = df_good_sensors['precip'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate statstics for PM values depending on rain (0 = no precipitation, 1 = precipitation)\n",
    "df_good_sensors.groupby('precip_bool').describe()[['PM2p5', 'PM10']].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When it's raining the maximum PM values are clearly smaller than without rain. For all other statistic values there values there is no clear dependency. As a consequence precipitation seems to have no real impact on PM values in Bremen hand Frankfurt.\n",
    "\n",
    "This observation can be explained with the comparable small PM concentration in the given cities. For Beijing it was shown that \"the washing process of rainfall strongly affects PM2.5, which decreased to 10â30 Î¼g/m3 with 5 mm of rainfall.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate \n",
    "df_delta_pm = pd.DataFrame()\n",
    "for location in df_good_sensors['location_id'].unique():\n",
    "    df_temp = df_good_sensors[df_good_sensors['location_id']==location][['location_id', 'timestamp', 'city', 'PM10', 'PM2p5', 'precip']]\n",
    "\n",
    "    df_temp['PM2p5_shifted_1'] = df_temp['PM2p5'].shift(periods=1)\n",
    "    df_temp['PM2p5_shifted_2'] = df_temp['PM2p5'].shift(periods=2)\n",
    "    df_temp['PM2p5_shifted_3'] = df_temp['PM2p5'].shift(periods=3)\n",
    "\n",
    "    df_temp['PM2p5_delta'] = df_temp['PM2p5'].shift(periods=1) - df_temp['PM2p5']\n",
    "    df_temp['PM2p5_delta_percent'] = (df_temp['PM2p5'].shift(periods=1) - df_temp['PM2p5']) / df_temp['PM2p5'] * 100\n",
    "\n",
    "    df_temp['PM2p5_delta_2'] = df_temp['PM2p5'].shift(periods=2) - df_temp['PM2p5']\n",
    "    df_temp['PM2p5_delta_2_percent'] = (df_temp['PM2p5'].shift(periods=2) - df_temp['PM2p5']) / df_temp['PM2p5'] * 100\n",
    "\n",
    "    df_temp['PM10_delta'] = df_temp['PM10'].shift(periods=1) - df_temp['PM10']\n",
    "    df_temp['PM10_delta_percent'] = (df_temp['PM10'].shift(periods=1) - df_temp['PM10']) / df_temp['PM10'] * 100\n",
    "\n",
    "    df_temp['PM2p5_rolling'] = df_temp['PM2p5'].rolling(window=5).mean()\n",
    "\n",
    "    df_delta_pm = pd.concat([df_delta_pm, df_temp])\n",
    "\n",
    "df_delta_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group precipitation by city and count occurences of precipitation intensity\n",
    "df_precip = df_delta_pm[['precip', 'city', 'location_id']].groupby(['city', 'precip']).count().reset_index().rename(columns={'location_id': 'count'})\n",
    "\n",
    "# calculate the observations per city\n",
    "sum_bremen = df_precip[df_precip['city']=='Bremen']['count'].sum()\n",
    "sum_frankfurt = df_precip[df_precip['city']=='Frankfurt']['count'].sum()\n",
    "\n",
    "# calculate the percentage of the count per precipitation intensity\n",
    "df_precip['percent'] = np.nan\n",
    "df_precip.loc[df_precip['city']=='Bremen', 'percent'] = df_precip.loc[df_precip['city']=='Bremen', 'count'] / sum_bremen * 100\n",
    "df_precip.loc[df_precip['city']=='Frankfurt', 'percent'] = df_precip.loc[df_precip['city']=='Frankfurt', 'count'] / sum_frankfurt * 100\n",
    "\n",
    "# plot precipitation\n",
    "plt.figure(figsize=(25, 10))\n",
    "g = sns.barplot(data=df_precip, x='precip', y='percent', hue='city')\n",
    "g.axes.tick_params(labelrotation=90)\n",
    "g.set_ylim(0, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In roughly 90 % of all hours in this data no rain occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "g = sns.scatterplot(data=df_delta_pm, x='precip', y='PM2p5_delta', hue='city')\n",
    "g.set_ylim(-50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "g = sns.scatterplot(data=df_delta_pm, x='precip', y='PM2p5_delta_percent', hue='city')\n",
    "g.set_ylim(-200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "mask = np.triu(np.ones_like(df_delta_pm.corr(), dtype=bool))\n",
    "sns.heatmap(df_delta_pm.corr(), annot=True, cmap=\"YlGnBu_r\", mask=mask, vmax=1, vmin=-1,fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precipitation doesn't show correlation with any of the investigated parameters. PM2.5 concentration seem not to decrease after raining, regardless the amount of rain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between PM2.5 and PM10 and autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate \n",
    "df_pm = pd.DataFrame()\n",
    "for location in df_good_sensors['location_id'].unique():\n",
    "    df_temp = df_good_sensors[df_good_sensors['location_id']==location][['location_id', 'timestamp', 'city', 'PM10', 'PM2p5']]\n",
    "\n",
    "    for i in range(24):\n",
    "        df_temp[f'PM2p5_shifted_{i+1}'] = df_temp['PM2p5'].shift(periods=i+1)\n",
    "\n",
    "    df_temp['PM2p5_PM10'] = df_temp['PM2p5'] / df_temp['PM10']\n",
    "\n",
    "    df_temp['PM2p5_PM10_rolling'] = df_temp['PM2p5_PM10'].rolling(5).mean()\n",
    "\n",
    "    df_pm = pd.concat([df_pm, df_temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "mask = np.triu(np.ones_like(df_pm.drop(['location_id', 'PM10', 'PM2p5_PM10'], axis=1).corr(), dtype=bool))\n",
    "sns.heatmap(df_pm.drop(['location_id', 'PM10', 'PM2p5_PM10'], axis=1).corr(), annot=True, cmap=\"YlGnBu_r\", mask=mask, vmax=1, vmin=0,fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the correlation decreases over time. In other words, PM2.5 concentrations that are closer together in a temporal manner are mor correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_pm, x='PM2p5_PM10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm[df_pm['PM2p5_PM10']==0][['PM2p5_PM10', 'PM2p5', 'PM10']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm[df_pm['PM2p5_PM10']>=1][['PM2p5_PM10', 'PM2p5', 'PM10']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df_pm['location_id'].unique()[:10]\n",
    "columns = 5\n",
    "rows = int(np.ceil((len(locations) - 1) / columns))\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(25, 80 / 13 * (len(locations)//5)))\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .25, wspace = .15, top = .93) # adjusts the space between the single subplots\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(locations):\n",
    "            g=sns.histplot(\n",
    "                data=df_pm[df_pm['location_id']==locations[row * columns + col]],\n",
    "                x='PM2p5_PM10',\n",
    "                ax=ax[row][col],\n",
    "            )\n",
    "            ax[row][col].set_xlim(-.05, 1.05)\n",
    "            ax[row][col].set_title(locations[row * columns + col], fontsize = 20)\n",
    "            ax[row][col].tick_params(labelrotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [this publication](https://www.frontiersin.org/articles/10.3389/fenvs.2021.692440/full) the authors characterized those PM2.5/PM10 histograms with a mode over 0.6 as anthropogenic. Using this classifications, in both cities are several locations that can be classified as anthropogenic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pm.loc[0,'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm['month'] = df_pm['timestamp'].dt.month\n",
    "df_pm['year'] = df_pm['timestamp'].dt.year\n",
    "\n",
    "df_pm_grouped_2021 = df_pm[['location_id', 'city', 'PM10', 'PM2p5', 'month', 'year']].groupby(['location_id', 'month', 'year', 'city']).mean().reset_index().query(\"year==2021\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(25, 10))\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "fig.subplots_adjust(hspace = .25, wspace = .15, top = .9) # adjusts the space between the single subplots\n",
    "fig.suptitle(\"Mean PM2.5 concentration per month\", fontsize=30)\n",
    "i = 0\n",
    "for city in df_pm_grouped_2021['city'].unique():\n",
    "    g=sns.barplot(\n",
    "        data=df_pm_grouped_2021[df_pm_grouped_2021['city']==city],\n",
    "        x='month',\n",
    "        y='PM2p5',\n",
    "        ax=ax[i],\n",
    "        color='b',\n",
    "    )\n",
    "    ax[i].set_title(city, fontsize = 20)\n",
    "    ax[i].set_ylim(0, 17)\n",
    "    ax[i].set_ylabel(\"PM2.5 in Âµg/m$^3$\")\n",
    "    ax[i].set_xlabel(\"Month\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean PM2.5 concentration per city shows a clear seasonality. The concentrations are rather high in winter and low in summer, which could be due to the increased energy need caused by heating in cold months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df_pm['location_id'].unique()[:10]\n",
    "columns = 1\n",
    "rows = int(np.ceil((len(locations) - 1) / columns))\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(30, (150/65)*len(locations)))\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "fig.subplots_adjust(hspace = .35, wspace = .15, top = .97) # adjusts the space between the single subplots\n",
    "fig.suptitle(\"PM2.5 / PM10 ratio over time (rolling average)\", fontsize=30)\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(locations):\n",
    "            g=sns.scatterplot(\n",
    "                data=df_pm[df_pm['location_id']==locations[row * columns + col]],\n",
    "                x='timestamp',\n",
    "                y='PM2p5_PM10_rolling',\n",
    "                ax=ax[row]#[col],\n",
    "            )\n",
    "\n",
    "            g.set_xlim(pd.to_datetime('2020-01-01'), pd.to_datetime('2021-12-31'))\n",
    "            g.set_ylim(-.1,1.1)\n",
    "            if row < rows-1:\n",
    "                g.set_xticklabels([])\n",
    "                g.set_xlabel('')\n",
    "            ax[row].set_title(f\"ID: {locations[row * columns + col]}\", fontsize = 20)\n",
    "            ax[row].set_ylabel(\"PM2.5 / PM10\", fontsize = 20)\n",
    "            # ax[row].tick_params(labelrotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "As we want to find general hyperparameters for both locations, we calculate the prior_scales for the use of weather data as regressors for both cities at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr_mean (feature, df):\n",
    "    \"\"\"calculates mean for a features given in df\n",
    "    \"\"\"\n",
    "    return df[feature].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['humidity_dwd', 'pressure_dwd', 'precip', 'wind_direction', 'wind_speed', 'temperature_dwd']\n",
    "# prepare DataFrame for results\n",
    "df_mean_features = pd.DataFrame(['mean', 'prior_scale'])\n",
    "\n",
    "# calc mean and add to DataFrame\n",
    "for i in features:\n",
    "    df_mean_features[i] = round(calc_corr_mean(i, df_PM2p5_correlations), 2)\n",
    "\n",
    "# calculate prior_scale and add to DataFrame\n",
    "sum_ = df_mean_features.iloc[0:1, 1:].abs().sum(axis=1)[0]\n",
    "for i in features:\n",
    "    df_mean_features.loc[1, i] = round(np.abs(df_mean_features.loc[0, i] / sum_), 2)\n",
    "df_mean_features.T\n",
    "\n",
    "# check if sum is 1\n",
    "#df_mean_features.iloc[1:2, 1:].abs().sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlations with mean values for whole data set\n",
    "df_PM2p5_correlations[['humidity_dwd', 'pressure_dwd', 'precip', 'wind_direction', 'wind_speed', 'temperature_dwd']].plot(figsize=(15,8))\n",
    "plt.title('                        Frankfurt                                                                          Bremen', fontsize=20)\n",
    "plt.xlabel('location_id', fontsize=20)\n",
    "plt.ylabel('correlation with PM2p5', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.ylim(-0.5,0.5)\n",
    "plt.xlim(0)\n",
    "plt.plot([124.5, 124.5], [-0.5, 0.5], linewidth=5, color='black')\n",
    "for i in features:\n",
    "    plt.plot([0, 182], [df_mean_features.loc[0, i], df_mean_features.loc[0, i]], linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How severe is PM pollution in Frankfurt and Bremen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# European Air Quality Index https://www.eea.europa.eu/themes/air/air-quality-index (source: Wikipedia)\n",
    "def pm2p5_bins(pm):\n",
    "    bins = {\n",
    "        '1-good': [0,10],                    # good\n",
    "        '2-fair': [10,20],                   # fair\n",
    "        '3-moderate': [20,25],                   # moderate\n",
    "        '4-poor': [25,50],                   # poor\n",
    "        '5-very poor': [50,75],                   # very poor\n",
    "        '6-extremely poor': [75,800],                  # extremely poor\n",
    "        '7-undefined': [800,2000]                 # undefined\n",
    "    }\n",
    "    for k,v in bins.items():\n",
    "        if v[0] <= pm < v[1]:\n",
    "            return k\n",
    "\n",
    "def pm10_bins(pm):\n",
    "    bins = {\n",
    "        '1-good': [0,20],                    # good\n",
    "        '2-fair': [20,40],                   # fair\n",
    "        '3-moderate': [40,50],                   # moderate\n",
    "        '4-poor': [50,100],                   # poor\n",
    "        '5-very poor': [100,150],                   # very poor\n",
    "        '6-extremely poor': [150,1200],                  # extremely poor\n",
    "        '7-undefined': [1200,2000]                 # undefined\n",
    "    }\n",
    "    for k,v in bins.items():\n",
    "        if v[0] <= pm < v[1]:\n",
    "            return k\n",
    "\n",
    "# add air quality to DataFrame        \n",
    "df_good_sensors[\"PM2p5_quality\"] = df_good_sensors[\"PM2p5\"].apply(pm2p5_bins)\n",
    "df_good_sensors[\"PM10_quality\"] = df_good_sensors[\"PM10\"].apply(pm10_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of PM  measurements depending on air quality per city\n",
    "# PM2p5\n",
    "pm2p5_quality_count = pd.DataFrame(df_good_sensors.query(\"city=='Bremen'\")['PM2p5_quality'].value_counts())\n",
    "pm2p5_quality_count['Frankfurt'] = pd.DataFrame(df_good_sensors.query(\"city=='Frankfurt'\")['PM2p5_quality'].value_counts())\n",
    "pm2p5_quality_count.reset_index(inplace=True)\n",
    "pm2p5_quality_count.columns = ['quality', 'PM2p5_Bremen', 'PM2p5_Frankfurt']\n",
    "pm2p5_quality_count['PM2p5_sum'] = pm2p5_quality_count['PM2p5_Bremen'] + pm2p5_quality_count['PM2p5_Frankfurt']\n",
    "#display(pm2p5_quality_count)\n",
    "\n",
    "# PM10\n",
    "pm10_quality_count = pd.DataFrame(df_good_sensors.query(\"city=='Bremen'\")['PM10_quality'].value_counts())\n",
    "pm10_quality_count['Frankfurt'] = pd.DataFrame(df_good_sensors.query(\"city=='Frankfurt'\")['PM10_quality'].value_counts())\n",
    "pm10_quality_count.reset_index(inplace=True)\n",
    "pm10_quality_count.columns = ['quality', 'PM10_Bremen', 'PM10_Frankfurt']\n",
    "pm10_quality_count['PM10_sum'] = pm10_quality_count['PM10_Bremen'] + pm10_quality_count['PM10_Frankfurt']\n",
    "#display(pm10_quality_count)\n",
    "\n",
    "# merge PM2p5 and PM10\n",
    "quality_absolute = pm2p5_quality_count.merge(pm10_quality_count, on='quality').sort_values('quality')\n",
    "display(quality_absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentages of PM  measurements depending on air quality per city\n",
    "# PM2p5\n",
    "percentage_PM2p5 = pd.DataFrame((pd.crosstab(index=[0], columns=df_good_sensors.query(\"city=='Bremen'\")['PM2p5_quality'], normalize=\"index\") * 100).round(2).iloc[0,:])\n",
    "percentage_PM2p5['Frankfurt'] = (pd.crosstab(index=[0], columns=df_good_sensors.query(\"city=='Frankfurt'\")['PM2p5_quality'], normalize=\"index\") * 100).round(2).iloc[0,:]\n",
    "percentage_PM2p5['sum'] = (pd.crosstab(index=[0], columns=df_good_sensors['PM2p5_quality'], normalize=\"index\") * 100).round(2).iloc[0,:]\n",
    "percentage_PM2p5.reset_index(inplace=True)\n",
    "percentage_PM2p5.columns = ['quality', 'PM2p5_Bremen', 'PM2p5_Frankfurt', 'PM2p5_sum']\n",
    "#display(percentage_PM2p5)\n",
    "\n",
    "# PM10\n",
    "percentage_PM10 = pd.DataFrame((pd.crosstab(index=[0], columns=df_good_sensors.query(\"city=='Bremen'\")['PM10_quality'], normalize=\"index\") * 100).round(2).iloc[0,:])\n",
    "percentage_PM10['Frankfurt'] = (pd.crosstab(index=[0], columns=df_good_sensors.query(\"city=='Frankfurt'\")['PM10_quality'], normalize=\"index\") * 100).round(2).iloc[0,:]\n",
    "percentage_PM10['sum'] = (pd.crosstab(index=[0], columns=df_good_sensors['PM10_quality'], normalize=\"index\") * 100).round(2).iloc[0,:]\n",
    "percentage_PM10.reset_index(inplace=True)\n",
    "percentage_PM10.columns = ['quality', 'PM10_Bremen', 'PM10_Frankfurt', 'PM10_sum']\n",
    "#display(percentage_PM10)\n",
    "\n",
    "quality_percentage = percentage_PM2p5.merge(percentage_PM10, on='quality')\n",
    "quality_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot percentages\n",
    "\n",
    "\n",
    "quality_percentage[['quality', 'PM2p5_Bremen', 'PM2p5_Frankfurt', 'PM2p5_sum']].plot(kind='bar', \n",
    "                    x='quality',\n",
    "                    stacked=False, \n",
    "                    colormap='tab10', # 'tab10' 'Set1' 'Dark2'\n",
    "                    figsize=(15, 6))\n",
    "plt.title('PM2.5')\n",
    "plt.ylim(0,100)\n",
    "plt.legend(['Bremen', 'Frankfurt', 'sum'])\n",
    "plt.ylabel('measurements (%)', fontsize=15)\n",
    "plt.xlabel('air quality', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "quality_percentage[['quality', 'PM10_Bremen', 'PM10_Frankfurt', 'PM10_sum']].plot(kind='bar', \n",
    "                    x='quality',\n",
    "                    stacked=False, \n",
    "                    colormap='tab10', # 'tab10' 'Set1' 'Dark2'\n",
    "                    figsize=(15, 6))\n",
    "plt.title('PM10')\n",
    "plt.ylim(0,100)\n",
    "plt.legend(['Bremen', 'Frankfurt', 'sum'])\n",
    "plt.ylabel('measurements (%)', fontsize=15)\n",
    "plt.xlabel('air quality', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of DWD weather data and PM2.5 values for Frankfurt and Bremen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of dwd weather data in comparison for Frankfurt and Bremen\n",
    "cmap = ['#4c72b0', '#dd8552'] # blue and orange \n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(25,16))\n",
    "    plt.subplot(2,3,1)\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='humidity_dwd', alpha=0.5, bins=40, hue='city', palette=cmap)\n",
    "    plt.xlim(0,100)\n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlabel('relative humidity (%)')\n",
    "    plt.ylabel('count')\n",
    "    \n",
    "    plt.subplot(2,3,4)\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='temperature_dwd', alpha=0.5, bins=60, hue='city', palette=cmap)\n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlim(-20, 40)\n",
    "    plt.xlabel('temperature (Â°C)')\n",
    "    plt.ylabel('count')\n",
    "\n",
    "    plt.subplot(2,3,3)\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='wind_speed', alpha=0.5, bins=60, hue='city', palette=cmap)\n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlim(0,15)\n",
    "    plt.xlabel('wind speed (m/s)')\n",
    "    plt.ylabel('count')\n",
    "        \n",
    "    plt.subplot(2,3,2)\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='pressure_dwd', alpha=0.5, bins=60, hue='city', palette=cmap)    \n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlim(970 , 1050)\n",
    "    plt.xlabel('air pressure (mbar)')\n",
    "    plt.ylabel('count')\n",
    "    \n",
    "    plt.subplot(2,3,5)\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='wind_direction', alpha=0.5, bins=30, hue='city', palette=cmap)\n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlim(0, 360)\n",
    "    plt.xlabel('wind direction (Â°)')\n",
    "    plt.ylabel('count')\n",
    "    \n",
    "    plt.subplot(2,3,6)\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='precip', alpha=0.5, bins=40, hue='city', palette=cmap)\n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.ylim(0,500)\n",
    "    plt.xlim(0, 10)\n",
    "    plt.xlabel('precipitation (?)')\n",
    "    plt.ylabel('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of dwd weather data in comparison for Frankfurt and Bremen (for presentation)\n",
    "cmap = ['#4c72b0', '#dd8552'] # blue and orange \n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    \n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='humidity_dwd', alpha=0.5, bins=40, hue='city', palette=cmap)\n",
    "    plt.xlim(0,100)\n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlabel('relative humidity in %')\n",
    "    plt.ylabel('counts')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id == 2 or location_id == 182\"), x='pressure_dwd', alpha=0.5, bins=60, hue='city', palette=cmap)    \n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlim(970 , 1050)\n",
    "    plt.xlabel('air pressure in mbar')\n",
    "    plt.ylabel('counts')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('../figures/histplot_dwd_data_per_city.png', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good_sensors.query(\"location_id > 125\").shape\n",
    "df_good_sensors.query(\"location_id < 55\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ['#4c72b0', '#dd8552'] # blue and orange for Frankfurt and  Bremen\n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    sns.histplot(data=df_good_sensors.query(\"location_id > 125 or location_id < 55\"), x='PM2p5', alpha=0.5, bins=1000, hue='city', palette=cmap)\n",
    "    plt.legend().remove() # no legend shown\n",
    "    plt.xlabel('PM$_{2.5}$ in Âµg/m$^3$')\n",
    "    plt.ylabel('count')\n",
    "    plt.xlim(0,50);\n",
    "    plt.savefig('../figures/PM2p5_hist.png', transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good_sensors['month'] =  df_good_sensors['timestamp'].dt.month\n",
    "df_good_sensors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "df_dwd_grouped = df_good_sensors[['location_id', 'city', 'PM10', 'PM2p5', 'month', 'humidity_dwd', 'precip', 'pressure_dwd', 'temperature_dwd', 'wind_direction', 'wind_speed']].groupby(['location_id', 'month', 'city']).median().reset_index()\n",
    "df_dwd_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(3,2,1)\n",
    "sns.barplot(data=df_dwd_grouped, x='month', y='pressure_dwd', hue='city')\n",
    "plt.ylabel('mean pressure (mbar)')\n",
    "plt.ylim(990, 1030)\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "sns.barplot(data=df_dwd_grouped, x='month', y='temperature_dwd', hue='city')\n",
    "plt.ylabel('mean temperature ()')\n",
    "#plt.ylim(990, 1020)\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "sns.barplot(data=df_dwd_grouped, x='month', y='wind_speed', hue='city')\n",
    "plt.ylabel('mean wind speed ()')\n",
    "#plt.ylim(990, 1020)\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "sns.barplot(data=df_dwd_grouped, x='month', y='humidity_dwd', hue='city')\n",
    "plt.ylabel('mean humidity ()')\n",
    "#plt.ylim(990, 1020)\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "sns.barplot(data=df_dwd_grouped, x='month', y='wind_direction', hue='city')\n",
    "plt.ylabel('mean wind direction ()')\n",
    "#plt.ylim(990, 1020)\n",
    "\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "sns.barplot(data=df_dwd_grouped, x='month', y='PM2p5', hue='city')\n",
    "plt.ylabel('mean PM2.5 ()')\n",
    "#plt.ylim(990, 1020)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "df_pm_grouped = df_pm[['location_id', 'city', 'PM10', 'PM2p5', 'month', 'year']].groupby(['location_id', 'month', 'year', 'city']).mean().reset_index()\n",
    "sns.barplot(data=df_pm_grouped, x='month', y='PM2p5', hue='city')\n",
    "plt.ylabel('mean PM2.5')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eff187d94a6d345964e064b31a9e6fc453a64676d5a266be90b3132f78586ec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
