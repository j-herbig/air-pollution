{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_theme()\n",
    "plt.rcParams.update({'figure.facecolor':'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and do basic formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed_sensor_dwd_train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# convert pressure to hPa\n",
    "df['pressure_sensors'] = df['pressure_sensors'] / 100\n",
    "df['pressure_std'] = df['pressure_std'] / 100\n",
    "\n",
    "# add sensor IDs\n",
    "df_location = df.groupby(['lat', 'lon']).count().reset_index()[['lat', 'lon']]\n",
    "df_location['location_id'] = df_location.index+1\n",
    "df = df.merge(df_location, on=['lat', 'lon'], how='left')\n",
    "\n",
    "# define lists with columns\n",
    "no_data_cols = ['location_id', 'timestamp', 'city', 'lat', 'lon']\n",
    "sc_cols = sorted(['PM10', 'PM2p5', 'PM10_std', 'PM2p5_std', 'pressure_sensors', 'temperature_sensors', 'humidity_sensors', 'pressure_std', 'temperature_std', 'humidity_std'])\n",
    "sc_cols_wo_std = [col for col in sc_cols if 'std' not in col]\n",
    "dwd_cols = sorted([col for col in df.columns if (col not in no_data_cols and col not in sc_cols)])\n",
    "std_cols = [col for col in sc_cols if 'std' in col]\n",
    "data_cols_wo_std = sc_cols_wo_std + dwd_cols\n",
    "data_cols = sc_cols + dwd_cols\n",
    "\n",
    "# reorganize columns: first non-data columns, then sorted data columns\n",
    "df = df.reindex(columns=no_data_cols + sc_cols + dwd_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save assignment of sensor_id to coordinates\n",
    "location_id_assignment = pd.DataFrame(\n",
    "    data={\n",
    "        'location_id': df['location_id'].unique()\n",
    "    }\n",
    ")\n",
    "for l in ['lat', 'lon']:\n",
    "    location_id_assignment[l] = location_id_assignment.apply(lambda x: df.loc[df['location_id']==x['location_id'], l].iloc[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of missing values, zeros and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the whole sc dataset\n",
    "df[sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the sc dataset for Frankfurt\n",
    "df[df['city']=='Frankfurt'][sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the sc dataset for Bremen\n",
    "df[df['city']=='Bremen'][sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM10: Mean is almost double of the 75th percentile -> Outliers raise the mean extremely </br>\n",
    "PM2.5: similar to PM10, but less extreme </br>\n",
    "humidity: al values (mean, 25th, 50th and 75th percentile) seem to be very large, the max value is above 100, what doesn't make any sense </br>\n",
    "pressure: assuming the units are Pa (1 bar = 100.000 Pa): min value is below 100 -> unrealistic, max value is also unrealistic (more than 60 bar) </br>\n",
    "temperature: std seems very high (54 °C), min and max value are unrealistic </br>\n",
    " </br>\n",
    " Bremen vs. Frankfurt </br>\n",
    " PM10 and PM2.5: std for Bremen is double of std for Frankfurt </br>\n",
    " humidity: 50th percentile of Bremen is already 99.9 % what seems quite high\n",
    " pressure and temperature: no obvious unrealistic observations besides the min and max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"missing values in each column\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].isna().sum()} ({round(df[col].isna().sum() / df.shape[0] * 100, 1)} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"value '0' in each column\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[df[col]==0][col].count()} ({round(df[df[col]==0][col].count() / df.shape[0] * 100, 1)} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan_and_0s(df: pd.DataFrame, cols: list = None) -> pd.DataFrame:\n",
    "    \"\"\"Counts zeros and nans per column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to search for zeros and nans.\n",
    "        cols (list, optional): List of columns, if no columns are specified all will be used. Defaults to None.\n",
    "        thresholds (dict, optional): Thresholds for further . Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing counts of zeros and nans.\n",
    "    \"\"\"\n",
    "    # use all columns af none were defined\n",
    "    if cols == None:\n",
    "        cols=df.columns\n",
    "    # make a new dataframe and put the defined column names in the first column\n",
    "    df_nan_0 = pd.DataFrame()\n",
    "    df_nan_0['data'] = cols\n",
    "    # calculate missing values and zeros as absolute value and share \n",
    "    df_nan_0['missing_values'] = [df[col].isna().sum() for col in cols]\n",
    "    df_nan_0['missing_values_share'] = [df[col].isna().sum() / df.shape[0] * 100 for col in cols]\n",
    "    df_nan_0['0_values'] = [df[df[col]==0][col].count() for col in cols]\n",
    "    df_nan_0['0_values_share'] = [df[df[col]==0][col].count() / df.shape[0] * 100 for col in cols]\n",
    "\n",
    "    # transpose the dataframe and use the original column names as column names\n",
    "    df_nan_0 = df_nan_0.set_index('data').T.reset_index()\n",
    "    df_nan_0.columns = [name if i>0 else 'metric' for i, name in enumerate(df_nan_0.columns)]\n",
    "    return df_nan_0\n",
    "\n",
    "\n",
    "# find missing values and zeros in the sc dataset\n",
    "df_data_analysis = count_nan_and_0s(df, data_cols)\n",
    "df_data_analysis.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics and columns to plot\n",
    "metrics = [\"missing_values_share\", \"0_values_share\"]\n",
    "ys = list(df_data_analysis.columns)\n",
    "ys.remove('metric')\n",
    "\n",
    "# define size of subplot\n",
    "columns = 4\n",
    "rows = int(np.ceil((len(df_data_analysis.columns) - 1) / columns))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "plt.suptitle(\"Data analysis of missing values and zeros\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(ys):\n",
    "        \n",
    "            # create a bar for each metric defined above for a column of ys list\n",
    "            sns.barplot(data=df_data_analysis[df_data_analysis['metric'].isin(metrics)], x='metric', y=ys[col + row * columns], ax=ax[row][col])\n",
    "            # set ylim to [0, 100] as we are plotting percentages\n",
    "            ax[row][col].set_ylim([0, 100])\n",
    "            # put the percentage above each plotted bar\n",
    "            ax[row][col].bar_label(ax[row][col].containers[0], fmt='%.1f')\n",
    "            # set the x, y and x-tick labels\n",
    "            ax[row][col].set_xlabel(\"\")\n",
    "            ax[row][col].set_ylabel(\"Share of values in %\")\n",
    "            ax[row][col].set_xticklabels(labels=[\"Missing values\", \"Zeros\"])\n",
    "            # use the column name with slight changes as subplot name\n",
    "            title = f\"{ys[col + row * columns]}\".replace('_', ' ').replace('std', 'std. dev.').replace('2p5', '2.5').capitalize()\n",
    "            ax[row][col].set_title(title, fontsize = 15);\n",
    "        else:\n",
    "            # delete not needed subplots\n",
    "            fig.delaxes(ax[row][col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to plot\n",
    "ys = data_cols_wo_std\n",
    "\n",
    "# define size of subplot\n",
    "columns = 3\n",
    "rows = int(np.ceil((len(ys)) / columns))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "plt.suptitle(\"Outlier analysis\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .7, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(ys):\n",
    "\n",
    "            # create a bar for each metric defined above for a column of ys list\n",
    "            sns.scatterplot(data=df, x='timestamp', y=ys[col + row * columns], ax=ax[row][col], alpha=.3)\n",
    "            # set the x, y and x-tick labels\n",
    "            ax[row][col].set_xlabel(ax[row][col].get_xlabel().capitalize())\n",
    "            ax[row][col].set_ylabel(ax[row][col].get_ylabel().capitalize())\n",
    "            # use the column name with slight changes as subplot name\n",
    "            title = f\"{ys[col + row * columns]}\".replace('_', ' ').replace('std', 'std. dev.').replace('2p5', '2.5').capitalize()\n",
    "            ax[row][col].set_title(title, fontsize = 15)\n",
    "            ax[row][col].tick_params(labelrotation=90)\n",
    "        else:\n",
    "            # delete not needed subplots\n",
    "            fig.delaxes(ax[row][col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few outliers in humidity, pressure and temperature which can be dropped by setting thresholds. </br>\n",
    "For PM10 and PM2.5 it is less obvious as the data is scattered all over the possible range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete unrealistic values and outliers for environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard thresholds based on physical estimations\n",
    "We can first have a look at the extreme values measured by Deutscher Wetterdienst to get an impression what range of values is realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['humidity_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['humidity_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['humidity_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['pressure_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['pressure_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['pressure_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['temperature_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['temperature_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['temperature_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lower and upper threshold\n",
    "thresholds_env = {\n",
    "    'humidity_sensors': (15, 100),\n",
    "    'pressure_sensors': (960, 1050),\n",
    "    'temperature_sensors': (-20, 60),\n",
    "}\n",
    "\n",
    "def del_hard_thresholds_env(df, thresholds_env=thresholds_env):\n",
    "    # delete values below lower and above upper threshold\n",
    "    for col, thresh in thresholds_env.items():\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df.iloc[df[col] <= thresh[0], list(df.columns).index(col)] = np.nan\n",
    "        df.iloc[df[col] >= thresh[1], list(df.columns).index(col)] = np.nan\n",
    "        print(f\"added {df[col].isna().sum() - nan_before} nans in {col}\")\n",
    "\n",
    "print(df['temperature_sensors'].isna().sum())\n",
    "del_hard_thresholds_env(df)\n",
    "print(df['temperature_sensors'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## values with std. dev. 'nan' or zero\n",
    "If the standard deviation is 'nan', there was no or only one observation. If the standard deviation is zero, there was no fluctuation in the measured value, what can be assumed to be a measurement error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete values for the defined columns if the standard deviation is zero or 'nan'\n",
    "cols_env = [\n",
    "    'temperature_sensors',\n",
    "    'humidity_sensors',\n",
    "    'pressure_sensors',\n",
    "]\n",
    "\n",
    "def del_std_nan_env(df, cols=cols_env):\n",
    "    for col in cols:\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df.loc[df[col.split('_')[0]+'_std']==0, col] = np.nan    \n",
    "        df.loc[df[col.split('_')[0]+'_std']==np.nan, col] = np.nan    \n",
    "        print(f\"added {df[col].isna().sum() - nan_before} nans in {col}\")\n",
    "\n",
    "print(df['temperature_sensors'].isna().sum())\n",
    "del_std_nan_env(df)\n",
    "print(df['temperature_sensors'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dynamic thresholds based on quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quantiles as threshold\n",
    "thresh = {\n",
    "    'temperature': (.01, .85),\n",
    "    'humidity': (.05, .95),\n",
    "    'pressure': (.05, .95),\n",
    "}\n",
    "\n",
    "\n",
    "def del_dynamic_threshold_env(df, thresh=thresh):\n",
    "# make a dataframe containing median, upper and lower threshold defined by the quantiles above\n",
    "    df_thresholds = df.groupby(['city', 'timestamp']).agg(\n",
    "        temp_median = pd.NamedAgg(column='temperature_sensors', aggfunc='median'), \n",
    "        temp_lower = pd.NamedAgg(column='temperature_sensors', aggfunc=lambda x: x.quantile(q=thresh['temperature'][0])),\n",
    "        temp_upper = pd.NamedAgg(column='temperature_sensors', aggfunc=lambda x: x.quantile(q=thresh['temperature'][1])),\n",
    "        hum_median = pd.NamedAgg(column='humidity_sensors', aggfunc='median'), \n",
    "        hum_lower = pd.NamedAgg(column='humidity_sensors', aggfunc=lambda x: x.quantile(q=thresh['humidity'][0])),\n",
    "        hum_upper = pd.NamedAgg(column='humidity_sensors', aggfunc=lambda x: x.quantile(q=thresh['humidity'][1])),\n",
    "        pres_median = pd.NamedAgg(column='pressure_sensors', aggfunc='median'), \n",
    "        pres_lower = pd.NamedAgg(column='pressure_sensors', aggfunc=lambda x: x.quantile(q=thresh['pressure'][0])),\n",
    "        pres_upper = pd.NamedAgg(column='pressure_sensors', aggfunc=lambda x: x.quantile(q=thresh['pressure'][1])),\n",
    "    ).reset_index()\n",
    "\n",
    "    # merge the thresholds with the sc dataframe\n",
    "    df = df.merge(df_thresholds, how='left', on=['city', 'timestamp'])\n",
    "\n",
    "    # replace values below lower threshold and above upper threshold with 'nan'\n",
    "    for col, thresholds in {\n",
    "        'temperature_sensors': ['temp_lower', 'temp_upper'],\n",
    "        'humidity_sensors': ['hum_lower', 'hum_upper'],\n",
    "        'pressure_sensors': ['pres_lower','pres_upper'],\n",
    "    }.items():\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df.loc[(df[col] < df[thresholds[0]]) | (df[col] > df[thresholds[1]]), col] = np.nan\n",
    "        print(f\"{df[col].isna().sum() - nan_before} nans added in {col}\")\n",
    "\n",
    "    # drop columns used for dynamic thresholding\n",
    "    df.drop([col for col in df_thresholds.columns if not col in no_data_cols], axis=1, inplace=True)\n",
    "\n",
    "print(df['temperature_sensors'].isna().sum())\n",
    "del_dynamic_threshold_env(df)\n",
    "print(df['temperature_sensors'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_thresholds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace values below lower threshold and above upper threshold with 'nan'\n",
    "# for col, thresholds in {\n",
    "#     'temperature_sensors': ['temp_lower', 'temp_upper'],\n",
    "#     'humidity_sensors': ['hum_lower', 'hum_upper'],\n",
    "#     'pressure_sensors': ['pres_lower','pres_upper'],\n",
    "# }.items():\n",
    "#     nan_before = df[col].isna().sum()\n",
    "#     df.loc[(df[col] < df[thresholds[0]]) | (df[col] > df[thresholds[1]]), col] = np.nan\n",
    "#     print(f\"{df[col].isna().sum() - nan_before} nans added in {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop columns used for dynamic thresholding\n",
    "# df.drop([col for col in df_thresholds.columns if not col in no_data_cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of cleaned data and comparison with dwd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sc_vs_dwd(city, columns=1, reduction=1):\n",
    "    # Plot dwd and sc data \n",
    "    # define size of subplot\n",
    "    rows = int(np.ceil(3 / columns))\n",
    "\n",
    "    fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "    plt.suptitle(f\"Comparison sensor data vs. dwd in {city}\", fontsize=20) # title of plot\n",
    "    fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "    plt.subplots_adjust(hspace = .2, wspace = .2, top = .95) # adjusts the space between the single subplots\n",
    "\n",
    "    # Plot humidity from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['humidity_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='humidity_sensors', ax=ax[0], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['humidity_dwd'].notna()) & (df['city']== city)], x='timestamp', y='humidity_dwd', color='red', alpha=.5, ax=ax[0], label='Deutscher Wetterdienst')\n",
    "    ax[0].set_ylabel('Relative Humidity in %')\n",
    "\n",
    "    # Plot pressure from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['pressure_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='pressure_sensors', ax=ax[1], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['pressure_dwd'].notna()) & (df['city']== city)], x='timestamp', y='pressure_dwd', color='red', alpha=.5, ax=ax[1], label='Deutscher Wetterdienst')\n",
    "    ax[1].set_ylabel('Pressure in hPa')\n",
    "\n",
    "    # Plot temperature from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['temperature_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='temperature_sensors', ax=ax[2], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['temperature_dwd'].notna()) & (df['city']== city)], x='timestamp', y='temperature_dwd', color='red', alpha=.5, ax=ax[2], label='Deutscher Wetterdienst')\n",
    "    ax[2].set_ylabel('Temperature in °C')\n",
    "\n",
    "    xlim_left = df['timestamp'].min()\n",
    "    xlim_right = df['timestamp'].max()\n",
    "\n",
    "    # capitalize axis titles and add legend\n",
    "    for i in range(3):\n",
    "        ax[i].legend(loc='lower right')\n",
    "        ax[i].set_xlabel(ax[i].get_xlabel().capitalize())\n",
    "        ax[i].set_xlim(xlim_left, xlim_right)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot comparison of data from both sources for Frankfurt\n",
    "# plot_sc_vs_dwd('Frankfurt')\n",
    "# plt.savefig(\"../figures/EDA_sc_vs_dwd_Frankfurt.png\", bbox_inches='tight')\n",
    "# plt.close()\n",
    "# ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDA_sc_vs_dwd_Frankfurt.png](../figures/EDA_sc_vs_dwd_Frankfurt.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot comparison of data from both sources for Bremen\n",
    "# plot_sc_vs_dwd('Bremen')\n",
    "# plt.savefig(\"../figures/EDA_sc_vs_dwd_Bremen.png\", bbox_inches='tight')\n",
    "# plt.close()\n",
    "# ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDA_sc_vs_dwd_Bremen.png](../figures/EDA_sc_vs_dwd_Bremen.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the distribution of measured temperatures in one day\n",
    "sns.histplot(data=df[(df['timestamp'] > '2020-07-01') & (df['timestamp'] < '2020-07-15')], x='temperature_sensors', bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of single locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by location_id and calculate the total number of hours with measurements, date of the first and of the last measurement\n",
    "location_grouped = df[(df['PM10'].notna()) & (df['PM2p5'].notna())][['location_id', 'timestamp']].\\\n",
    "    groupby(['location_id']).\\\n",
    "        agg(\n",
    "                hours = pd.NamedAgg(column='timestamp', aggfunc='count'), \n",
    "                date_min = pd.NamedAgg(column='timestamp', aggfunc='min'),\n",
    "                date_max = pd.NamedAgg(column='timestamp', aggfunc='max')\n",
    "            ).\\\n",
    "            reset_index().\\\n",
    "                sort_values('hours', ascending=False)\n",
    "\n",
    "location_grouped['date_min'] = pd.to_datetime(location_grouped['date_min'])\n",
    "location_grouped['date_max'] = pd.to_datetime(location_grouped['date_max'])\n",
    "location_grouped['period_length'] = location_grouped['date_max'] - location_grouped['date_min'] + pd.Timedelta(days=1)\n",
    "location_grouped['hours_per_day'] = location_grouped['hours'] / location_grouped['period_length'].dt.days\n",
    "location_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of hours that were measured at each location\n",
    "plt.figure(figsize=(25, 10))\n",
    "g = sns.barplot(data=location_grouped, x='location_id', y='hours', order=location_grouped.sort_values('hours', ascending=False)['location_id'])\n",
    "g.set_xlabel(g.get_xlabel().capitalize().replace('_', ' '))\n",
    "g.set_ylabel(g.get_ylabel().capitalize())\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of hours per day measured per location\n",
    "plt.figure(figsize=(25, 10))\n",
    "g = sns.barplot(data=location_grouped.sort_values('hours_per_day', ascending=False), x='location_id', y='hours_per_day', order=location_grouped.sort_values('hours_per_day', ascending=False)['location_id'])\n",
    "g.set_xlabel(g.get_xlabel().capitalize().replace('_', ' '))\n",
    "g.set_ylabel(g.get_ylabel().capitalize().replace('_', ' '))\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of locations: {location_grouped.shape[0]}\")\n",
    "print('Locations with the least hours of measurement:')\n",
    "location_grouped.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_grouped[['hours', 'hours_per_day']].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some sensor locations which delivered data only for few hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_PM(df):\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 20))\n",
    "    plt.suptitle(\"Sensors per City\", fontsize=20) # title of plot\n",
    "    fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "    plt.subplots_adjust(hspace = .5, wspace = .2, top = .9) # adjusts the space between the single subplots\n",
    "\n",
    "    # get ids match them with the cities\n",
    "    labels_frankfurt = set(df.query(\"city=='Frankfurt'\")['location_id'])\n",
    "    labels_bremen = set(df.query(\"city=='Bremen'\")['location_id'])\n",
    "\n",
    "    # plot PM10 data of Frankfurt\n",
    "    sns.lineplot(data=df[df['city']=='Frankfurt'][::10], x='timestamp', y='PM10', hue='location_id', ax=ax1, legend=False)\n",
    "    ax1.legend(labels=labels_frankfurt) # assign a unique color to every id\n",
    "    ax1.set_title('Frankfurt - PM10', fontsize = 15) # set title and font size\n",
    "    ax1.legend([], [], frameon=False) # hide legend\n",
    "\n",
    "    # plot PM2.5 data for Frankfurt\n",
    "    sns.lineplot(data=df[df['city']=='Frankfurt'][::10], x='timestamp', y='PM2p5', hue='location_id', ax=ax2, legend=False)\n",
    "    ax2.legend(labels=labels_frankfurt)\n",
    "    ax2.set_title('Frankfurt - PM2.5', fontsize = 15)\n",
    "    ax2.legend([], [], frameon=False)\n",
    "\n",
    "    # plot PM10 data for Bremen\n",
    "    sns.lineplot(data=df[df['city']=='Bremen'][::10], x='timestamp', y='PM10', hue='location_id', ax=ax3, legend=False)\n",
    "    ax3.legend(labels=labels_bremen)\n",
    "    ax3.set_title('Bremen - PM10', fontsize = 15)\n",
    "    ax3.legend([], [], frameon=False)\n",
    "\n",
    "    # plot PM2.5 data for Bremen\n",
    "    sns.lineplot(data=df[df['city']=='Bremen'][::10], x='timestamp', y='PM2p5', hue='location_id', ax=ax4, legend=False)\n",
    "    ax4.legend(labels=labels_bremen)\n",
    "    ax4.set_title('Bremen - PM2.5', fontsize = 15)\n",
    "    ax4.legend([], [], frameon=False)\n",
    "\n",
    "plot_all_PM(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example location (location_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get location_id's occuring in Frankfurt\n",
    "ids_frankfurt = df.query(\"city=='Frankfurt'\")['location_id'].unique()\n",
    "\n",
    "# plot PM10, PM2.5 and humidity of one location\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='PM10', color='b', alpha=.5)\n",
    "sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='PM2p5', color=\"r\", alpha=.5, ax=ax)\n",
    "ax2 = ax.twinx() # add second y-axis\n",
    "sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='humidity_sensors', color=\"g\", alpha=.5, ax=ax2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap for one single location\n",
    "sns.heatmap(df[df['location_id']==ids_frankfurt[0]][sc_cols_wo_std].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe containing timestamps of one year with resolution of one hour\n",
    "one_year_full = pd.DataFrame()\n",
    "one_year_full['timestamp'] = pd.date_range(\"2021-03-01\", \"2022-02-28 23:00:00\", freq=\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add observations of one location to that dataframe\n",
    "one_year_full_2 = pd.merge(one_year_full, df[df['location_id']==ids_frankfurt[0]], how='left', on='timestamp')\n",
    "print(f\"{one_year_full_2['PM10'].isna().sum()} missing values in PM10\")\n",
    "print(f\"{one_year_full_2['PM2p5'].isna().sum()} missing values in PM2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of observations where PM10 value is 'NaN'\n",
    "missing_index = one_year_full_2.index[one_year_full_2['PM10'].isna()].tolist()\n",
    "\n",
    "missing_periods = [] # list for periods of missing values\n",
    "i = 0 # index for loop\n",
    "start = None # start of a period\n",
    "previous = None # index of the previous loop\n",
    "\n",
    "\n",
    "while i < len(missing_index):\n",
    "    # if start is None, it is the first loop\n",
    "    if start == None:\n",
    "        start = previous = missing_index[i]\n",
    "        i += 1\n",
    "        continue\n",
    "    # if the current index is the previous index + 1, we are still moving within a closed period\n",
    "    if missing_index[i] == previous+1:\n",
    "        previous = missing_index[i]\n",
    "        i += 1\n",
    "        continue\n",
    "    # else one period is over and another one is starting\n",
    "    else:\n",
    "        # add the closed period to the list of missing periods\n",
    "        missing_periods.append(\n",
    "            (one_year_full_2['timestamp'][start], \n",
    "            one_year_full_2['timestamp'][previous], \n",
    "            one_year_full_2['timestamp'][previous] - one_year_full_2['timestamp'][start] + pd.Timedelta(1, 'hour'))\n",
    "        )\n",
    "        start = previous = missing_index[i]\n",
    "        i += 1\n",
    "# add the last period to the list\n",
    "missing_periods.append(\n",
    "    (one_year_full_2['timestamp'][start], \n",
    "    one_year_full_2['timestamp'][previous], \n",
    "    one_year_full_2['timestamp'][previous]  - one_year_full_2['timestamp'][start] + pd.Timedelta(1, 'hour'))\n",
    ")\n",
    "\n",
    "# print the periods of missing PM10 values and their duration\n",
    "p = 0\n",
    "for start, end, duration in missing_periods:\n",
    "    p += 1\n",
    "    print(f\"Period of missing values #{p}:\\n\\tstart: {start}\\n\\tend: {end}\\n\\tduration: {duration}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dynamic thresholds for PM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL: Calculate a dynamic median per hour for all sensors in a city. If a value is for example three times the median it is estimated to be an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pm(df: pd.DataFrame, cols: list=['PM10', 'PM2p5'], factor: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"deletes outliers for the given columns and considerung their timestamps and cities which are larger than factor times the median\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe\n",
    "        cols (list): columns to clean\n",
    "        factor (int, optional): factor that is used to calculate the threshold for keeping or deleting data. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    for col in df.columns:\n",
    "        if 'threshold' in col:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # define a list for saving the thresholds\n",
    "    thresholds = []\n",
    "\n",
    "    # for each city in the dataframe make a dataframe with timestamps\n",
    "    for city in df['city'].unique():\n",
    "        df_cur = df[df['city'] == city]\n",
    "        df_threshold = pd.DataFrame(\n",
    "            data={\n",
    "                'timestamp': df_cur['timestamp'].unique(), \n",
    "                'city': city\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # for each timestamp calculate the median and threshold (factor * median)\n",
    "        for col in cols:\n",
    "            df_threshold[col+'_median'] = df_threshold.apply(lambda x: df_cur[(df_cur['timestamp'] == x['timestamp'])][col].median(), axis=1)\n",
    "            df_threshold[col+'_threshold'] = factor * df_threshold[col+'_median']\n",
    "        thresholds.append(df_threshold)\n",
    "\n",
    "    # concatenate all thresholds\n",
    "    df_thresholds = pd.DataFrame()\n",
    "    for df_threshold in thresholds:\n",
    "        df_thresholds = pd.concat([df_thresholds, df_threshold])\n",
    "    \n",
    "    # merge thresholds with original dataframe on timestamp and city \n",
    "    df = df.merge(df_thresholds, how='left', on=['timestamp', 'city'])\n",
    "    \n",
    "    # delete values if they are above the threshold and print number of deleted values\n",
    "    for col in cols:\n",
    "        nan_before = df[col].isna().sum()\n",
    "        df[col] = df.apply(lambda x: x[col] if x[col] <= x[col+'_threshold'] else np.nan, axis=1)\n",
    "        print(f\"{df[col].isna().sum() - nan_before} NaNs added in {col}\")\n",
    "\n",
    "    # for col in cols:\n",
    "    #     df.drop([col+'_threshold'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = clean_pm(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_PM(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PM_data_per_location(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): Dataframe containing data of PM sensors\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing one dataframe per city and PM sensor\n",
    "    \"\"\"\n",
    "    # make dataframe containing the timestamps\n",
    "    df_missing_values_bremen_pm10 = pd.DataFrame(\n",
    "        data={\n",
    "            'timestamp': df['timestamp'].unique(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # copy that dataframe for every combination of PM sensor and city\n",
    "    df_missing_values_bremen_pm2p5 = df_missing_values_bremen_pm10.copy()\n",
    "    df_missing_values_frankfurt_pm10 = df_missing_values_bremen_pm10.copy()\n",
    "    df_missing_values_frankfurt_pm2p5 = df_missing_values_bremen_pm10.copy()\n",
    "\n",
    "    # add sensor data for every location in Bremen\n",
    "    for location in df.loc[df['city'] == 'Bremen', 'location_id'].unique():\n",
    "        df_missing_values_bremen_pm10 = pd.merge(df_missing_values_bremen_pm10, df.loc[df['location_id']==location, ['timestamp','PM10']], on='timestamp')\n",
    "        df_missing_values_bremen_pm10.rename(columns={'PM10': location}, inplace=True) # rename the new column using the location_id\n",
    "        df_missing_values_bremen_pm10.set_index('timestamp', inplace=True) # use timestamps as index\n",
    "\n",
    "        df_missing_values_bremen_pm2p5 = pd.merge(df_missing_values_bremen_pm2p5, df.loc[df['location_id']==location, ['timestamp','PM2p5']], on='timestamp')\n",
    "        df_missing_values_bremen_pm2p5.rename(columns={'PM2p5': location}, inplace=True)\n",
    "        df_missing_values_bremen_pm2p5.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # do the same for Frankfurt\n",
    "    for location in df.loc[df['city'] == 'Frankfurt', 'location_id'].unique():\n",
    "        df_missing_values_frankfurt_pm10 = pd.merge(df_missing_values_frankfurt_pm10, df.loc[df['location_id']==location, ['timestamp','PM10']], on='timestamp')\n",
    "        df_missing_values_frankfurt_pm10.rename(columns={'PM10': location}, inplace=True)\n",
    "        df_missing_values_frankfurt_pm10.set_index('timestamp', inplace=True)\n",
    "\n",
    "        df_missing_values_frankfurt_pm2p5 = pd.merge(df_missing_values_frankfurt_pm2p5, df.loc[df['location_id']==location, ['timestamp','PM2p5']], on='timestamp')\n",
    "        df_missing_values_frankfurt_pm2p5.rename(columns={'PM2p5': location}, inplace=True)\n",
    "        df_missing_values_frankfurt_pm2p5.set_index('timestamp', inplace=True)\n",
    "    return  df_missing_values_bremen_pm10, df_missing_values_bremen_pm2p5, df_missing_values_frankfurt_pm10, df_missing_values_frankfurt_pm2p5\n",
    "\n",
    "\n",
    "df_missing_values_bremen_pm10, df_missing_values_bremen_pm2p5, df_missing_values_frankfurt_pm10, df_missing_values_frankfurt_pm2p5 = get_PM_data_per_location(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 15))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 15))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 15))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop sensors with only few data in the past year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../data/df_backup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sns.set_theme()\n",
    "# plt.rcParams.update({'figure.facecolor':'white'})\n",
    "\n",
    "# df = pd.read_csv(\"../data/df_backup.csv\", index_col=0)\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_share_of_missing_values(df: pd.DataFrame, start_time: str):\n",
    "    # Get the total number of observations possible in the past year\n",
    "    observations_of_interest = df[(df['location_id'] == df['location_id'].unique()[0]) & (df['timestamp'] >= pd.to_datetime(start_time))].shape[0]\n",
    "\n",
    "    # make a dataframe to store missing values per location\n",
    "    missing_values = pd.DataFrame(columns=['location_id', 'city', 'PM10_missing', 'PM2p5_missing'])\n",
    "\n",
    "    # get missing values for every location\n",
    "    for location in df['location_id'].unique():\n",
    "        # filter for location\n",
    "        df_cur = df[(df['location_id'] == location) & (df['timestamp'] >= pd.to_datetime('2021-01-01'))][['city', 'PM10', 'PM2p5']]\n",
    "        \n",
    "        # create a new entry in the dataframe containing location_id, city and share of missing values\n",
    "        new_entry = {\n",
    "            'location_id': int(location),\n",
    "            'city': df_cur['city'].iloc[0],\n",
    "            'PM10_missing': df_cur['PM10'].isna().sum() / observations_of_interest,\n",
    "            'PM2p5_missing': df_cur['PM2p5'].isna().sum() / observations_of_interest,\n",
    "        }\n",
    "        missing_values = missing_values.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # cast location_id to int\n",
    "    missing_values['location_id'] = missing_values['location_id'].astype(int) \n",
    "    return missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_values = get_share_of_missing_values(df, '2021-01-01')\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,1,figsize=(20,15))\n",
    "plt.suptitle(\"Missing values per city and sensor\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .4, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "i=0\n",
    "# plot share of missing values for every city and PM sensor\n",
    "for city in missing_values['city'].unique():\n",
    "    for col in ['PM10_missing', 'PM2p5_missing']:\n",
    "        sns.barplot(\n",
    "            data=missing_values[missing_values['city']==city],\n",
    "            x='location_id',\n",
    "            y=col,\n",
    "            order=missing_values[missing_values['city']==city].sort_values(col, ascending=False)['location_id'], # sort by missing values\n",
    "            ax=ax[i]\n",
    "        )\n",
    "        ax[i].tick_params(labelrotation=90) # rotate x tick labels\n",
    "        ax[i].set_title(city + ' - ' + col.split('_')[0].replace('p', '.')) # set a title (City - Sensor)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the IDs of good sensors having less than 25 % missing values in PM2.5\n",
    "good_sensors = missing_values.query(\"PM2p5_missing < 0.25\")['location_id']\n",
    "good_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data of those good sensors\n",
    "def use_good_sensors_only(df, good_sensors=good_sensors):\n",
    "    df_good_sensors = df[df['location_id'].\\\n",
    "        isin(good_sensors)].\\\n",
    "            drop([col for col in df.columns if ('median' in col or 'threshold' in col)], axis=1)\n",
    "    return df_good_sensors\n",
    "\n",
    "df_good_sensors = use_good_sensors_only(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df_good_sensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test data\n",
    "df_test = pd.read_csv(\"../data/processed_sensor_dwd_test.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign location IDs according to coordinates\n",
    "df_test['location_id'] = df_test.apply(\n",
    "    lambda x: location_id_assignment.\\\n",
    "        loc[(location_id_assignment['lat'] == x['lat']) & (location_id_assignment['lon'] == x['lon']), 'location_id'].\\\n",
    "            iloc[0], \n",
    "            axis=1\n",
    ")\n",
    "df_test['location_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime\n",
    "df_test['timestamp'] = pd.to_datetime(df_test['timestamp'])\n",
    "\n",
    "# sort columns\n",
    "df_test = df_test.reindex(columns=no_data_cols + sc_cols + dwd_cols)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers of environmental parameters by different mechanisms\n",
    "print(\"hard thresholds\")\n",
    "del_hard_thresholds_env(df_test)\n",
    "\n",
    "print(\"constant values\")\n",
    "del_std_nan_env(df_test)\n",
    "\n",
    "print(\"dnyamic thersholds\")\n",
    "del_dynamic_threshold_env(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all PM data\n",
    "plot_all_PM(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get missing values of PM data per sensor\n",
    "df_missing_values_bremen_pm10_test, \\\n",
    "df_missing_values_bremen_pm2p5_test, \\\n",
    "df_missing_values_frankfurt_pm10_test, \\\n",
    "df_missing_values_frankfurt_pm2p5_test = get_PM_data_per_location(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean PM data\n",
    "df_test = clean_pm(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data of sensors marked as good\n",
    "df_good_sensors_test = use_good_sensors_only(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.shape)\n",
    "print(df_good_sensors_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of 'good sensors' should be identical to locations in test dataframe\n",
    "print(len(good_sensors))\n",
    "df_good_sensors_test['location_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all PM data per location\n",
    "plot_all_PM(df_good_sensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get missing values per sensor\n",
    "df_missing_values_bremen_pm10_test, \\\n",
    "df_missing_values_bremen_pm2p5_test, \\\n",
    "df_missing_values_frankfurt_pm10_test, \\\n",
    "df_missing_values_frankfurt_pm2p5_test = get_PM_data_per_location(df_good_sensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get share of missing values in the cleaned test dataframe\n",
    "missing_values_test = get_share_of_missing_values(df_good_sensors_test, \"2021-01-01\")\n",
    "\n",
    "# make a series of good sensors in test data (less than 75 % missing in PM2.5)\n",
    "good_sensors_test = missing_values_test.query(\"PM2p5_missing < 0.25\")['location_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bad sensors in test dataframe (more than 75 % of PM2.5 data missing)\n",
    "bad_sensors = []\n",
    "for location in list(good_sensors):\n",
    "    if location not in list(good_sensors_test):\n",
    "        bad_sensors.append(location)\n",
    "\n",
    "print(len(bad_sensors))\n",
    "bad_sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update dataframes using only 'good sensors' and save cleaned train and test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update train dataframe according to good sensors in test data\n",
    "df_good_sensors = use_good_sensors_only(df, good_sensors_test)\n",
    "\n",
    "# save train data for good sensors\n",
    "df_good_sensors.to_csv(\"../data/cleaned_sensors_dwd_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test dataframe containing only good sensors\n",
    "df_good_sensors_test = use_good_sensors_only(df_test, good_sensors_test)\n",
    "\n",
    "# save test data\n",
    "df_good_sensors_test.to_csv(\"../data/cleaned_sensors_dwd_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last check of missing data in the final dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_values_bremen_pm10_test, \\\n",
    "df_missing_values_bremen_pm2p5_test, \\\n",
    "df_missing_values_frankfurt_pm10_test, \\\n",
    "df_missing_values_frankfurt_pm2p5_test = get_PM_data_per_location(df_good_sensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5_test.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt - Cleaned Test Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_values_bremen_pm10, \\\n",
    "df_missing_values_bremen_pm2p5, \\\n",
    "df_missing_values_frankfurt_pm10, \\\n",
    "df_missing_values_frankfurt_pm2p5 = get_PM_data_per_location(df_good_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Bremen - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Bremen\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_bremen_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Bremen - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM10 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm10.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM10 - Frankfurt - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values per id for PM2.5 in Frankfurt\n",
    "plt.figure(figsize=(30, 10))\n",
    "g = sns.heatmap(df_missing_values_frankfurt_pm2p5.isna().T.sort_index(), cbar_kws={'label': 'Missing Data'})\n",
    "g.set_title('PM2.5 - Frankfurt - Cleaned Train Data', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eff187d94a6d345964e064b31a9e6fc453a64676d5a266be90b3132f78586ec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
