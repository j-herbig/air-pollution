{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_theme()\n",
    "plt.rcParams.update({'figure.facecolor':'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and do basic formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed_sensor_dwd.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# convert pressure to hPa\n",
    "df['pressure_sensors'] = df['pressure_sensors'] / 100\n",
    "df['pressure_std'] = df['pressure_std'] / 100\n",
    "\n",
    "# add sensor IDs\n",
    "df_location = df.groupby(['lat', 'lon']).count().reset_index()[['lat', 'lon']]\n",
    "df_location['location_id'] = df_location.index+1\n",
    "df = df.merge(df_location, on=['lat', 'lon'], how='left')\n",
    "\n",
    "# define lists with columns\n",
    "no_data_cols = ['location_id', 'timestamp', 'city', 'lat', 'lon']\n",
    "sc_cols = sorted(['PM10', 'PM2p5', 'PM10_std', 'PM2p5_std', 'pressure_sensors', 'temperature_sensors', 'humidity_sensors', 'pressure_std', 'temperature_std', 'humidity_std'])\n",
    "sc_cols_wo_std = [col for col in sc_cols if 'std' not in col]\n",
    "dwd_cols = sorted([col for col in df.columns if (col not in no_data_cols and col not in sc_cols)])\n",
    "std_cols = [col for col in sc_cols if 'std' in col]\n",
    "data_cols_wo_std = sc_cols_wo_std + dwd_cols\n",
    "data_cols = sc_cols + dwd_cols\n",
    "\n",
    "# reorganize columns: first non-data columns, then sorted data columns\n",
    "df = df.reindex(columns=no_data_cols + sc_cols + dwd_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of missing values, zeros and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the whole sc dataset\n",
    "df[sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the sc dataset for Frankfurt\n",
    "df[df['city']=='Frankfurt'][sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the sc dataset for Bremen\n",
    "df[df['city']=='Bremen'][sc_cols_wo_std].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM10: Mean is almost double of the 75th percentile -> Outliers raise the mean extremely </br>\n",
    "PM2.5: similar to PM10, but less extreme </br>\n",
    "humidity: al values (mean, 25th, 50th and 75th percentile) seem to be very large, the max value is above 100, what doesn't make any sense </br>\n",
    "pressure: assuming the units are Pa (1 bar = 100.000 Pa): min value is below 100 -> unrealistic, max value is also unrealistic (more than 60 bar) </br>\n",
    "temperature: std seems very high (54 Â°C), min and max value are unrealistic </br>\n",
    " </br>\n",
    " Bremen vs. Frankfurt </br>\n",
    " PM10 and PM2.5: std for Bremen is double of std for Frankfurt </br>\n",
    " humidity: 50th percentile of Bremen is already 99.9 % what seems quite high\n",
    " pressure and temperature: no obvious unrealistic observations besides the min and max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"missing values in each column\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].isna().sum()} ({round(df[col].isna().sum() / df.shape[0] * 100, 1)} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"value '0' in each column\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[df[col]==0][col].count()} ({round(df[df[col]==0][col].count() / df.shape[0] * 100, 1)} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan_and_0s(df: pd.DataFrame, cols: list = None) -> pd.DataFrame:\n",
    "    \"\"\"Counts zeros and nans per column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to search for zeros and nans.\n",
    "        cols (list, optional): List of columns, if no columns are specified all will be used. Defaults to None.\n",
    "        thresholds (dict, optional): Thresholds for further . Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing counts of zeros and nans.\n",
    "    \"\"\"\n",
    "    # use all columns af none were defined\n",
    "    if cols == None:\n",
    "        cols=df.columns\n",
    "    # make a new dataframe and put the defined column names in the first column\n",
    "    df_nan_0 = pd.DataFrame()\n",
    "    df_nan_0['data'] = cols\n",
    "    # calculate missing values and zeros as absolute value and share \n",
    "    df_nan_0['missing_values'] = [df[col].isna().sum() for col in cols]\n",
    "    df_nan_0['missing_values_share'] = [df[col].isna().sum() / df.shape[0] * 100 for col in cols]\n",
    "    df_nan_0['0_values'] = [df[df[col]==0][col].count() for col in cols]\n",
    "    df_nan_0['0_values_share'] = [df[df[col]==0][col].count() / df.shape[0] * 100 for col in cols]\n",
    "\n",
    "    # transpose the dataframe and use the original column names as column names\n",
    "    df_nan_0 = df_nan_0.set_index('data').T.reset_index()\n",
    "    df_nan_0.columns = [name if i>0 else 'metric' for i, name in enumerate(df_nan_0.columns)]\n",
    "    return df_nan_0\n",
    "\n",
    "\n",
    "# find missing values and zeros in the sc dataset\n",
    "df_data_analysis = count_nan_and_0s(df, data_cols)\n",
    "df_data_analysis.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics and columns to plot\n",
    "metrics = [\"missing_values_share\", \"0_values_share\"]\n",
    "ys = list(df_data_analysis.columns)\n",
    "ys.remove('metric')\n",
    "\n",
    "# define size of subplot\n",
    "columns = 4\n",
    "rows = int(np.ceil((len(df_data_analysis.columns) - 1) / columns))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "plt.suptitle(\"Data analysis of missing values and zeros\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(ys):\n",
    "        \n",
    "            # create a bar for each metric defined above for a column of ys list\n",
    "            sns.barplot(data=df_data_analysis[df_data_analysis['metric'].isin(metrics)], x='metric', y=ys[col + row * columns], ax=ax[row][col])\n",
    "            # set ylim to [0, 100] as we are plotting percentages\n",
    "            ax[row][col].set_ylim([0, 100])\n",
    "            # put the percentage above each plotted bar\n",
    "            ax[row][col].bar_label(ax[row][col].containers[0], fmt='%.1f')\n",
    "            # set the x, y and x-tick labels\n",
    "            ax[row][col].set_xlabel(\"\")\n",
    "            ax[row][col].set_ylabel(\"Share of values in %\")\n",
    "            ax[row][col].set_xticklabels(labels=[\"Missing values\", \"Zeros\"])\n",
    "            # use the column name with slight changes as subplot name\n",
    "            title = f\"{ys[col + row * columns]}\".replace('_', ' ').replace('std', 'std. dev.').replace('2p5', '2.5').capitalize()\n",
    "            ax[row][col].set_title(title, fontsize = 15);\n",
    "        else:\n",
    "            # delete not needed subplots\n",
    "            fig.delaxes(ax[row][col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to plot\n",
    "ys = data_cols_wo_std\n",
    "\n",
    "# define size of subplot\n",
    "columns = 3\n",
    "rows = int(np.ceil((len(ys)) / columns))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "plt.suptitle(\"Outlier analysis\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .7, wspace = .2, top = .93) # adjusts the space between the single subplots\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        if col + row * columns < len(ys):\n",
    "\n",
    "            # create a bar for each metric defined above for a column of ys list\n",
    "            sns.scatterplot(data=df, x='timestamp', y=ys[col + row * columns], ax=ax[row][col], alpha=.3)\n",
    "            # set the x, y and x-tick labels\n",
    "            ax[row][col].set_xlabel(ax[row][col].get_xlabel().capitalize())\n",
    "            ax[row][col].set_ylabel(ax[row][col].get_ylabel().capitalize())\n",
    "            # use the column name with slight changes as subplot name\n",
    "            title = f\"{ys[col + row * columns]}\".replace('_', ' ').replace('std', 'std. dev.').replace('2p5', '2.5').capitalize()\n",
    "            ax[row][col].set_title(title, fontsize = 15)\n",
    "            ax[row][col].tick_params(labelrotation=90)\n",
    "        else:\n",
    "            # delete not needed subplots\n",
    "            fig.delaxes(ax[row][col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few outliers in humidity, pressure and temperature which can be dropped by setting thresholds. </br>\n",
    "For PM10 and PM2.5 it is less obvious as the data is scattered all over the possible range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete unrealistic values and outliers for environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard thresholds based on physical estimations\n",
    "We can first have a look at the extreme values measured by Deutscher Wetterdienst to get an impression what range of values is realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['humidity_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['humidity_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['humidity_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['pressure_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['pressure_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['pressure_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['temperature_dwd'].max())\n",
    "print(df.query(\"city == 'Frankfurt'\")['temperature_dwd'].min())\n",
    "print(df.query(\"city == 'Bremen'\")['temperature_dwd'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds_part = {\n",
    "#     'PM10': (0, 1000),\n",
    "#     'PM2p5': (0, 500),\n",
    "# }\n",
    "# set lower and upper threshold\n",
    "thresholds_env = {\n",
    "    'humidity_sensors': (15, 100),\n",
    "    'pressure_sensors': (960, 1050),\n",
    "    'temperature_sensors': (-20, 60),\n",
    "}\n",
    "\n",
    "# delete values below lower and above upper threshold\n",
    "for col, thresh in thresholds_env.items():\n",
    "    nan_before = df[col].isna().sum()\n",
    "    df.iloc[df[col] <= thresh[0], list(df.columns).index(col)] = np.nan\n",
    "    df.iloc[df[col] >= thresh[1], list(df.columns).index(col)] = np.nan\n",
    "    print(f\"added {df[col].isna().sum() - nan_before} nans in {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## values with std. dev. 'nan' or zero\n",
    "If the standard deviation is 'nan', there was no or only one observation. If the standard deviation is zero, there was no fluctuation in the measured value, what can be assumed to be a measurement error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete values for the defined columns if the standard deviation is zero or 'nan'\n",
    "for col in [\n",
    "    'temperature_sensors',\n",
    "    'humidity_sensors',\n",
    "    'pressure_sensors',\n",
    "]:\n",
    "    nan_before = df[col].isna().sum()\n",
    "    df.loc[df[col.split('_')[0]+'_std']==0, col] = np.nan    \n",
    "    df.loc[df[col.split('_')[0]+'_std']==np.nan, col] = np.nan    \n",
    "    print(f\"added {df[col].isna().sum() - nan_before} nans in {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dynamic thresholds based on quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quantiles as threshold\n",
    "thresh = {\n",
    "    'temperature': (.01, .85),\n",
    "    'humidity': (.05, .95),\n",
    "    'pressure': (.05, .95),\n",
    "}\n",
    "\n",
    "# make a dataframe containing median, upper and lower threshold defined by the quantiles above\n",
    "df_thresholds = df.groupby(['city', 'timestamp']).agg(\n",
    "    temp_median = pd.NamedAgg(column='temperature_sensors', aggfunc='median'), \n",
    "    temp_lower = pd.NamedAgg(column='temperature_sensors', aggfunc=lambda x: x.quantile(q=thresh['temperature'][0])),\n",
    "    temp_upper = pd.NamedAgg(column='temperature_sensors', aggfunc=lambda x: x.quantile(q=thresh['temperature'][1])),\n",
    "    hum_median = pd.NamedAgg(column='humidity_sensors', aggfunc='median'), \n",
    "    hum_lower = pd.NamedAgg(column='humidity_sensors', aggfunc=lambda x: x.quantile(q=thresh['humidity'][0])),\n",
    "    hum_upper = pd.NamedAgg(column='humidity_sensors', aggfunc=lambda x: x.quantile(q=thresh['humidity'][1])),\n",
    "    pres_median = pd.NamedAgg(column='pressure_sensors', aggfunc='median'), \n",
    "    pres_lower = pd.NamedAgg(column='pressure_sensors', aggfunc=lambda x: x.quantile(q=thresh['pressure'][0])),\n",
    "    pres_upper = pd.NamedAgg(column='pressure_sensors', aggfunc=lambda x: x.quantile(q=thresh['pressure'][1])),\n",
    ").reset_index()\n",
    "\n",
    "# merge the thresholds with the sc dataframe\n",
    "df = df.merge(df_thresholds, how='left', on=['city', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values below lower threshold and above upper threshold with 'nan'\n",
    "for col, thresholds in {\n",
    "    'temperature_sensors': ['temp_lower', 'temp_upper'],\n",
    "    'humidity_sensors': ['hum_lower', 'hum_upper'],\n",
    "    'pressure_sensors': ['pres_lower','pres_upper'],\n",
    "}.items():\n",
    "    nan_before = df[col].isna().sum()\n",
    "    df.loc[(df[col] < df[thresholds[0]]) | (df[col] > df[thresholds[1]]), col] = np.nan\n",
    "    print(f\"{df[col].isna().sum() - nan_before} nans added in {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns used for dynamic thresholding\n",
    "df.drop([col for col in df_thresholds.columns if not col in no_data_cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of cleaned data and comparison with dwd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sc_vs_dwd(city, columns=1, reduction=1):\n",
    "    # Plot dwd and sc data \n",
    "    # define size of subplot\n",
    "    rows = int(np.ceil(3 / columns))\n",
    "\n",
    "    fig, ax = plt.subplots(rows, columns, figsize=(20,20)) # create subplots\n",
    "    plt.suptitle(f\"Comparison sensor data vs. dwd in {city}\", fontsize=20) # title of plot\n",
    "    fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "    plt.subplots_adjust(hspace = .2, wspace = .2, top = .95) # adjusts the space between the single subplots\n",
    "\n",
    "    # Plot humidity from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['humidity_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='humidity_sensors', ax=ax[0], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['humidity_dwd'].notna()) & (df['city']== city)], x='timestamp', y='humidity_dwd', color='red', alpha=.5, ax=ax[0], label='Deutscher Wetterdienst')\n",
    "    ax[0].set_ylabel('Relative Humidity in %')\n",
    "\n",
    "    # Plot pressure from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['pressure_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='pressure_sensors', ax=ax[1], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['pressure_dwd'].notna()) & (df['city']== city)], x='timestamp', y='pressure_dwd', color='red', alpha=.5, ax=ax[1], label='Deutscher Wetterdienst')\n",
    "    ax[1].set_ylabel('Pressure in hPa')\n",
    "\n",
    "    # Plot temperature from both datasets vs time\n",
    "    sns.scatterplot(data=df[(df['temperature_sensors'].notna()) & (df['city'] == city)][::reduction], x='timestamp', y='temperature_sensors', ax=ax[2], label='Sensor Community')\n",
    "    sns.lineplot(data=df[(df['temperature_dwd'].notna()) & (df['city']== city)], x='timestamp', y='temperature_dwd', color='red', alpha=.5, ax=ax[2], label='Deutscher Wetterdienst')\n",
    "    ax[2].set_ylabel('Temperature in Â°C')\n",
    "\n",
    "    xlim_left = df['timestamp'].min()\n",
    "    xlim_right = df['timestamp'].max()\n",
    "\n",
    "    # capitalize axis titles and add legend\n",
    "    for i in range(3):\n",
    "        ax[i].legend(loc='lower right')\n",
    "        ax[i].set_xlabel(ax[i].get_xlabel().capitalize())\n",
    "        ax[i].set_xlim(xlim_left, xlim_right)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot comparison of data from both sources for Frankfurt\n",
    "# plot_sc_vs_dwd('Frankfurt')\n",
    "# plt.savefig(\"../figures/EDA_sc_vs_dwd_Frankfurt.png\", bbox_inches='tight')\n",
    "# plt.close()\n",
    "# ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDA_sc_vs_dwd_Frankfurt.png](../figures/EDA_sc_vs_dwd_Frankfurt.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot comparison of data from both sources for Bremen\n",
    "# plot_sc_vs_dwd('Bremen')\n",
    "# plt.savefig(\"../figures/EDA_sc_vs_dwd_Bremen.png\", bbox_inches='tight')\n",
    "# plt.close()\n",
    "# ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDA_sc_vs_dwd_Bremen.png](../figures/EDA_sc_vs_dwd_Bremen.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the distribution of measured temperatures in one day\n",
    "sns.histplot(data=df[(df['timestamp'] > '2020-07-01') & (df['timestamp'] < '2020-07-15')], x='temperature_sensors', bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of single locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by location_id and calculate the total number of hours with measurements, date of the first and of the last measurement\n",
    "location_grouped = df[(df['PM10'].notna()) & (df['PM2p5'].notna())][['location_id', 'timestamp']].\\\n",
    "    groupby(['location_id']).\\\n",
    "        agg(\n",
    "                hours = pd.NamedAgg(column='timestamp', aggfunc='count'), \n",
    "                date_min = pd.NamedAgg(column='timestamp', aggfunc='min'),\n",
    "                date_max = pd.NamedAgg(column='timestamp', aggfunc='max')\n",
    "            ).\\\n",
    "            reset_index().\\\n",
    "                sort_values('hours', ascending=False)\n",
    "\n",
    "location_grouped['date_min'] = pd.to_datetime(location_grouped['date_min'])\n",
    "location_grouped['date_max'] = pd.to_datetime(location_grouped['date_max'])\n",
    "location_grouped['period_length'] = location_grouped['date_max'] - location_grouped['date_min'] + pd.Timedelta(days=1)\n",
    "location_grouped['hours_per_day'] = location_grouped['hours'] / location_grouped['period_length'].dt.days\n",
    "location_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of hours that were measured at each location\n",
    "plt.figure(figsize=(25, 10))\n",
    "g = sns.barplot(data=location_grouped, x='location_id', y='hours', order=location_grouped.sort_values('hours', ascending=False)['location_id'])\n",
    "g.set_xlabel(g.get_xlabel().capitalize().replace('_', ' '))\n",
    "g.set_ylabel(g.get_ylabel().capitalize())\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of hours per day measured per location\n",
    "plt.figure(figsize=(25, 10))\n",
    "g = sns.barplot(data=location_grouped.sort_values('hours_per_day', ascending=False), x='location_id', y='hours_per_day', order=location_grouped.sort_values('hours_per_day', ascending=False)['location_id'])\n",
    "g.set_xlabel(g.get_xlabel().capitalize().replace('_', ' '))\n",
    "g.set_ylabel(g.get_ylabel().capitalize().replace('_', ' '))\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of locations: {location_grouped.shape[0]}\")\n",
    "print('Locations with the least hours of measurement:')\n",
    "location_grouped.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_grouped[['hours', 'hours_per_day']].describe().T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some sensor locations which delivered data only for few hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 20))\n",
    "plt.suptitle(\"Sensors per City\", fontsize=20) # title of plot\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .9) # adjusts the space between the single subplots\n",
    "\n",
    "labels_frankfurt = set(df.query(\"city=='Frankfurt'\")['location_id'])\n",
    "labels_bremen = set(df.query(\"city=='Bremen'\")['location_id'])\n",
    "\n",
    "sns.lineplot(data=df[df['city']=='Frankfurt'][::10], x='timestamp', y='PM10', hue='location_id', ax=ax1, legend=False)\n",
    "ax1.legend(labels=labels_frankfurt)\n",
    "ax1.set_title('Frankfurt - PM10', fontsize = 15)\n",
    "ax1.legend([], [], frameon=False)\n",
    "\n",
    "sns.lineplot(data=df[df['city']=='Frankfurt'][::10], x='timestamp', y='PM2p5', hue='location_id', ax=ax2, legend=False)\n",
    "ax2.legend(labels=labels_frankfurt)\n",
    "ax2.set_title('Frankfurt - PM2.5', fontsize = 15)\n",
    "ax2.legend([], [], frameon=False)\n",
    "\n",
    "sns.lineplot(data=df[df['city']=='Bremen'][::10], x='timestamp', y='PM10', hue='location_id', ax=ax3, legend=False)\n",
    "ax3.legend(labels=labels_bremen)\n",
    "ax3.set_title('Bremen - PM10', fontsize = 15)\n",
    "ax3.legend([], [], frameon=False)\n",
    "\n",
    "sns.lineplot(data=df[df['city']=='Bremen'][::10], x='timestamp', y='PM2p5', hue='location_id', ax=ax4, legend=False)\n",
    "ax4.legend(labels=labels_bremen)\n",
    "ax4.set_title('Bremen - PM2.5', fontsize = 15)\n",
    "ax4.legend([], [], frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example location (location_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get location_id's occuring in Frankfurt\n",
    "ids_frankfurt = df.query(\"city=='Frankfurt'\")['location_id'].unique()\n",
    "\n",
    "# plot PM10, PM2.5 and humidity of one location\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='PM10', color='b', alpha=.5)\n",
    "sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='PM2p5', color=\"r\", alpha=.5, ax=ax)\n",
    "ax2 = ax.twinx() # add second y-axis\n",
    "sns.lineplot(data=df[df['location_id']==ids_frankfurt[0]], x='timestamp', y='humidity_sensors', color=\"g\", alpha=.5, ax=ax2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap for one single location\n",
    "sns.heatmap(df[df['location_id']==ids_frankfurt[0]][sc_cols_wo_std].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe containing timestamps of one year with resolution of one hour\n",
    "one_year_full = pd.DataFrame()\n",
    "one_year_full['timestamp'] = pd.date_range(\"2021-03-01\", \"2022-02-28 23:00:00\", freq=\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add observations of one location to that dataframe\n",
    "one_year_full_2 = pd.merge(one_year_full, df[df['location_id']==ids_frankfurt[0]], how='left', on='timestamp')\n",
    "print(f\"{one_year_full_2['PM10'].isna().sum()} missing values in PM10\")\n",
    "print(f\"{one_year_full_2['PM2p5'].isna().sum()} missing values in PM2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of observations where PM10 value is 'NaN'\n",
    "missing_index = one_year_full_2.index[one_year_full_2['PM10'].isna()].tolist()\n",
    "\n",
    "missing_periods = [] # list for periods of missing values\n",
    "i = 0 # index for loop\n",
    "start = None # start of a period\n",
    "previous = None # index of the previous loop\n",
    "\n",
    "\n",
    "while i < len(missing_index):\n",
    "    # if start is None, it is the first loop\n",
    "    if start == None:\n",
    "        start = previous = missing_index[i]\n",
    "        i += 1\n",
    "        continue\n",
    "    # if the current index is the previous index + 1, we are still moving within a closed period\n",
    "    if missing_index[i] == previous+1:\n",
    "        previous = missing_index[i]\n",
    "        i += 1\n",
    "        continue\n",
    "    # else one period is over and another one is starting\n",
    "    # add the closed period to the list of missing periods\n",
    "    else:\n",
    "        # print(start, previous)\n",
    "        missing_periods.append(\n",
    "            (one_year_full_2['timestamp'][start], \n",
    "            one_year_full_2['timestamp'][previous], \n",
    "            one_year_full_2['timestamp'][previous] - one_year_full_2['timestamp'][start] + pd.Timedelta(1, 'hour'))\n",
    "        )\n",
    "        start = previous = missing_index[i]\n",
    "        i += 1\n",
    "# add the last period to the list\n",
    "missing_periods.append(\n",
    "    (one_year_full_2['timestamp'][start], \n",
    "    one_year_full_2['timestamp'][previous], \n",
    "    one_year_full_2['timestamp'][previous]  - one_year_full_2['timestamp'][start] + pd.Timedelta(1, 'hour'))\n",
    ")\n",
    "\n",
    "# print the periods of missing PM10 values and their duration\n",
    "p = 0\n",
    "for start, end, duration in missing_periods:\n",
    "    p += 1\n",
    "    print(f\"Period of missing values #{p}:\\n\\tstart: {start}\\n\\tend: {end}\\n\\tduration: {duration}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dynamic thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL: Calculate a dynamic median over a certain period for all sensors in a city. If a value is for example twice the median it is estimated to be an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eff187d94a6d345964e064b31a9e6fc453a64676d5a266be90b3132f78586ec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
