{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # dataset handling\n",
    "import numpy as np\n",
    "import geopandas as gpd  # geodataset handling\n",
    "from keplergl import KeplerGl  # geospatial visualization \n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.serialize import model_from_json, model_to_json\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing latest Sensor and Meteomatics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load meteomatics data for the cities of interest \n",
    "cities = ['Bremen', 'Frankfurt']\n",
    "# set the day of the downloaded data\n",
    "day = datetime.datetime(2022, 3, 20).date()\n",
    "weather = {}\n",
    "\n",
    "for city in cities: \n",
    "    weather[city] = pd.read_csv(f'../data/Meteomatics/auto_processed_weather_forecast_{city}_{day}.csv')\n",
    "    weather[city].timestamp = pd.to_datetime(weather[city].timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../data/cleaned_sensors_dwd_2022-03-28.csv', index_col=0)\n",
    "df.timestamp = pd.to_datetime(df.timestamp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soon, this will be automated, once the meteomatics data is consistent in terms of the starting timestamps\n",
    "df_train = df[df.timestamp <= '2022-03-20 8:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Prophet on all available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set locations\n",
    "locations = list(df.location_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "seasonality_mode='additive'\n",
    "yearly_seasonality=True\n",
    "weekly_seasonality=True\n",
    "daily_seasonality=True\n",
    "\n",
    "growth='logistic'\n",
    "n_changepoints=25\n",
    "changepoint_prior_scale = 0.6  #default 0.05\n",
    "\n",
    "temp_flag = True\n",
    "press_flag = True\n",
    "windsp_flag = True\n",
    "winddir_flag = True\n",
    "precip_flag = True\n",
    "\n",
    "temp_prior_scale = 0.3\n",
    "press_prior_scale = 0.3\n",
    "windsp_prior_scale = 0.3\n",
    "winddir_prior_scale = 0.3\n",
    "precip_prior_scale = 0.3\n",
    "\n",
    "if not temp_flag:\n",
    "    temp_prior_scale = None\n",
    "if not press_flag:\n",
    "    press_prior_scale = None\n",
    "if not windsp_flag:\n",
    "    windsp_prior_scale = None\n",
    "if not winddir_flag:\n",
    "    winddir_prior_scale = None\n",
    "if not precip_flag:\n",
    "    precip_prior_scale = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "limits = pd.DataFrame(index=locations, columns=['cap', 'floor'])\n",
    "for location in locations:\n",
    "    # reduce data to one location\n",
    "    df_prophet_reg = df_train.query(f'location_id == {location}')[['timestamp','PM2p5', 'temperature', 'pressure', 'wind_speed', 'wind_direction', 'precip']] \n",
    "    df_prophet_reg = df_prophet_reg.sort_values(['timestamp'], axis=0)\n",
    "\n",
    "    # rename columns to expected format for prophet\n",
    "    df_prophet_reg.rename(columns={'timestamp': 'ds', 'PM2p5': 'y', 'temperature': 'temp', 'pressure': 'press', 'wind_speed': 'windsp', 'wind_direction': 'winddir', 'precip': 'precip'}, inplace=True)\n",
    "\n",
    "    # prophet can not handle nans in dataframe\n",
    "    df_prophet_reg.dropna(inplace=True, subset=['temp', 'press', 'windsp', 'precip', 'winddir'])\n",
    "\n",
    "    df_average = df_train.query(f'location_id == {location}')[['timestamp','PM2p5']]\n",
    "    df_average['PM2p5_average'] = (df_average.PM2p5.shift(2) + df_average.PM2p5.shift(1) + df_average.PM2p5 + df_average.PM2p5.shift(-1) + df_average.PM2p5.shift(-2)) / 5\n",
    "\n",
    "    # add cap column for to set growth = logistic\n",
    "    cap = df_average.PM2p5_average.quantile(0.99)\n",
    "    floor = df_average.PM2p5_average.min()\n",
    "    \n",
    "    limits.loc[location, 'cap'] = cap\n",
    "    limits.loc[location, 'floor'] = floor\n",
    "\n",
    "    df_prophet_reg['cap'] =  cap \n",
    "    df_prophet_reg['floor'] = floor\n",
    "\n",
    "    model = Prophet(seasonality_mode=seasonality_mode, yearly_seasonality=yearly_seasonality, weekly_seasonality=weekly_seasonality, daily_seasonality=daily_seasonality,\n",
    "        growth=growth,n_changepoints=n_changepoints, changepoint_prior_scale=changepoint_prior_scale)\n",
    "    \n",
    "    # add regressors \n",
    "    model.add_regressor('temp', standardize=True, prior_scale=temp_prior_scale)\n",
    "    model.add_regressor('press', standardize=True, prior_scale=press_prior_scale)\n",
    "    model.add_regressor('windsp', standardize=True, prior_scale=windsp_prior_scale)\n",
    "    model.add_regressor('winddir', standardize=True, prior_scale=winddir_prior_scale)\n",
    "    model.add_regressor('precip', standardize=True, prior_scale=precip_prior_scale)\n",
    "\n",
    "    # fit model \n",
    "    model.fit(df_prophet_reg)\n",
    "\n",
    "    # save model in dictionary \n",
    "    models[location] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill future dataframe with meteomatics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regressor_column(ds, train_col, test_col, location_id):\n",
    "    \"\"\"Get a regressor of train or test data for corresponding timestamp\n",
    "\n",
    "    Args:\n",
    "        ds (datetime): timestamp\n",
    "        train_col (string): column name of regressor in train data\n",
    "        test_col (string): column name of regressor in test data\n",
    "\n",
    "    Returns:\n",
    "        float: regressor value for given timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    if ds in df_prophet_reg['ds'].values:\n",
    "        return df_prophet_reg[df_prophet_reg['ds'] == ds][train_col].values[0]\n",
    "    elif ds in weather['Frankfurt']['timestamp'].values:\n",
    "        if location_id <= 124:\n",
    "            return weather['Frankfurt'][(weather['Frankfurt']['timestamp'] == ds)][test_col].values[0]\n",
    "        else: \n",
    "            return weather['Bremen'][(weather['Bremen']['timestamp'] == ds)][test_col].values[0]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = {}\n",
    "for location in locations: \n",
    "    model = models[location]\n",
    "    future = model.make_future_dataframe(periods=168, freq='H')\n",
    "    \n",
    "    future['temp'] = future['ds'].apply(create_regressor_column, args=('temp', 'temperature', location))\n",
    "    future['press'] = future['ds'].apply(create_regressor_column, args=('press', 'pressure', location))\n",
    "    future['windsp'] = future['ds'].apply(create_regressor_column, args=('windsp', 'wind_speed', location))\n",
    "    future['winddir'] = future['ds'].apply(create_regressor_column, args=('winddir', 'wind_direction', location))\n",
    "    future['precip'] = future['ds'].apply(create_regressor_column, args=('precip', 'precip', location))\n",
    "\n",
    "    future['cap'] = limits.loc[location, 'cap']\n",
    "    future['floor'] = limits.loc[location, 'floor']\n",
    "\n",
    "    # drop nans\n",
    "    future.dropna(inplace=True)\n",
    "    # predict\n",
    "    forecasts[location] = model.predict(future)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_dict_to_df(forecast: dict) -> pd.DataFrame:\n",
    "    \"\"\"Convert forecast dictionary to dataframe\n",
    "\n",
    "    Args:\n",
    "        forecast (dict): forecast dictionary from all locations' prophet models\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all forecasts\n",
    "    \"\"\"\n",
    "    forecasts = pd.DataFrame(columns=['ds', 'yhat', 'location_id'])\n",
    "\n",
    "    for location in list(forecast.keys()):\n",
    "        forecast[location]['location_id'] = location\n",
    "        forecasts = pd.concat([forecasts, forecast[location]], axis=0)\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models and forecasts for later use\n",
    "for location_id in locations:\n",
    "    model_json = model_to_json(models[location_id])\n",
    "    with open(f'../models/{day}/{location_id}_model_{day}.json', 'w') as f:\n",
    "        json.dump(model_json, f)\n",
    "    forecasts[location_id].to_csv(f'../models/{day}/{location_id}_forecast_{day}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine forecasts into one dataframe\n",
    "df_forecasts = forecast_dict_to_df(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df[(df.timestamp >= df_forecasts.ds.min()) & (df.timestamp <= df_forecasts.ds.max())][['location_id', 'timestamp', 'PM2p5']]\n",
    "df_eval.rename(columns={'timestamp': 'ds'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse = df_forecasts[['location_id','ds', 'yhat']].merge(df_eval, on=['location_id', 'ds'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_per_location(df_rmse: pd.DataFrame, forecast_horizon=168):\n",
    "    \"\"\"Calculate RMSE per location\n",
    "\n",
    "    Args: \n",
    "        df_rmse (pd.DataFrame): dataframe with y_true and y_hat values, timestamp and location_id\n",
    "        forecast_horizon (int): forecast horizon (in hours)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with RMSE train and test values per location\n",
    "    \"\"\"\n",
    "    locs = df_rmse.location_id.unique().tolist()\n",
    "    rmse = pd.DataFrame(columns=['RMSE_train', 'RMSE_test', 'PM2p5_mean'], index=locs)\n",
    "\n",
    "    for loc in locs: \n",
    "        df_loc = df_rmse[df_rmse.location_id == loc]\n",
    "        df_loc.dropna(inplace=True)\n",
    "        rmse.loc[loc, 'RMSE_train'] = mean_squared_error(df_loc['PM2p5'][:-forecast_horizon], df_loc['yhat'][:-forecast_horizon], squared=False)\n",
    "        rmse.loc[loc, 'RMSE_test'] = mean_squared_error(df_loc['PM2p5'][-forecast_horizon:], df_loc['yhat'][-forecast_horizon:], squared= False)\n",
    "        rmse.loc[loc, 'PM2p5_mean'] = df_loc['PM2p5'][:-forecast_horizon].mean()\n",
    "\n",
    "    rmse.reset_index(inplace=True)\n",
    "    rmse.rename(columns={'index': 'location_id'}, inplace=True)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE per location for 7 days (168 hours)\n",
    "rmse_locations = rmse_per_location(df_rmse)\n",
    "rmse_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_locations.sort_values(by='location_id').plot(kind='scatter', figsize=(25,15), x='PM2p5_mean', y='RMSE_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_locations.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with Kepler.gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GPS data and merge with forecast dataframe since kepler.gl needs GPS data\n",
    "df_gps = df[['location_id', 'lat', 'lon']].groupby(['location_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kepler = df_forecasts.merge(df_gps, how='left', on='location_id')[['ds', 'yhat', 'location_id', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kepler.dropna(subset=['yhat'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dummies\n",
    "timestamps = pd.Series(df_kepler['ds'].unique(), name='ds')\n",
    "\n",
    "dummies = pd.DataFrame(data={\n",
    "    'location_id': -1,\n",
    "    'lat': [0, 90],\n",
    "    'lon': [0, 90],\n",
    "    'yhat': [0, 50]\n",
    "})\n",
    "\n",
    "dummies = dummies.merge(timestamps, how='cross')\n",
    "\n",
    "df_kepler = pd.concat([df_kepler, dummies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust plotting time horizon\n",
    "df_kepler = df_kepler.sort_values('ds').query('ds > \"2022-03\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for PM2p5 for Hexbin plotting\n",
    "pm2p5_bins = np.append(0,np.arange(0, 50, 5))\n",
    "pm2p5_labels = pm2p5_bins\n",
    "pm2p5_labels[0] = -1\n",
    "pm2p5_bins = np.append(pm2p5_bins, 1000)\n",
    "pm2p5_bins[0] = -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pm2p5_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kepler['PM2p5_bins'] = pd.cut(df_kepler['yhat'], bins=pm2p5_bins, labels=pm2p5_labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the geo DataFrame\n",
    "gdf_sensors = gpd.GeoDataFrame(\n",
    "    df_kepler, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        x=df_kepler['lon'],\n",
    "        y=df_kepler['lat']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Datetime column (Kepler is funny about datetimes)\n",
    "gdf_sensors['timestamp'] = pd.to_datetime(gdf_sensors['ds'])\n",
    "gdf_sensors['timestamp'] = gdf_sensors['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Selecting only columns we need\n",
    "gdf_sensors = gdf_sensors[[\n",
    "    'yhat', 'lon', 'lat', 'geometry', 'timestamp',  'PM2p5_bins', 'location_id'\n",
    "]]\n",
    "\n",
    "gdf_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_sensors[gdf_sensors.yhat < 0].location_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run config.py\n",
    "map_config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepler_map = KeplerGl(\n",
    "    height=800,\n",
    "    data={\n",
    "        'Sensors': gdf_sensors,\n",
    "    }, config=map_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepler_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export for Kepler.gl or Unfolded online use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kepler.to_csv('../data/kepler.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a077374ed4dbb64161d40ed2104c7e33247b48512da5a01adf0d791950a02fa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
