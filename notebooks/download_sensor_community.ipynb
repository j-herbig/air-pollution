{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated .csv download\n",
    "Automated download of sensor data from [Sensor Community Archive](https://archive.sensor.community/csv_per_month/).</br>\n",
    "Define `MONTH_START`, `MONTH_COUNT` and `SENSORS` for specifying the files that should be downloaded.</br>\n",
    "Define `WAIT_BETWEEN_DOWNLOADS` to define the waiting time between one download started and the net begins. A random number between the two defined will be used.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MONTH_START = \"2020-01\" # Start month in the format yyyy-mm\n",
    "MONTH_COUNT = 1 # sensor data will be downloaded for this amount of months\n",
    "URL = \"https://archive.sensor.community/csv_per_month/\"\n",
    "ROOT_DIR = os.path.join(os.curdir, \"../data\", \"\")\n",
    "WAIT_BETWEEN_DOWNLOADS = (0, 5)\n",
    "SENSORS = [\n",
    "    'bme280', \n",
    "    # 'bmp180', \n",
    "    'bmp280', \n",
    "    'dht22',\n",
    "    # 'ds18b20', \n",
    "    # 'hpm', \n",
    "    # 'htu21d', \n",
    "    # 'pms1003', \n",
    "    # 'pms3003', \n",
    "    # 'pms5003', \n",
    "    # 'pms6003', \n",
    "    # 'pms7003', \n",
    "    # 'ppd42ns', \n",
    "    'sds011',\n",
    "]\n",
    "LAT_RANGE = [\n",
    "    (53.013, 53.1456), \n",
    "    (50.030681, 50.205692),\n",
    "    ] # Bremen: (53.013, 53.1456), Frankfurt: (50.030681, 50.205692)\n",
    "LON_RANGE = [\n",
    "    (8.67, 8.9334), \n",
    "    (8.430634, 8.919868),\n",
    "    ] # Bremen: (8.67, 8.9334), Frankfurt: (8.430634, 8.919868)\n",
    "\n",
    "def write_to_log(log_file, *args):\n",
    "    \"\"\"writes text to the defined log file\n",
    "\n",
    "    Args:\n",
    "        log_file: path to the log file\n",
    "        *args: one or more strings that are written to the log file\n",
    "    \"\"\"\n",
    "    with open(log_file, 'a') as log:\n",
    "        for text in args:\n",
    "            log.write(text)\n",
    "\n",
    "\n",
    "script_start = time.time()\n",
    "\n",
    "# make log file if it doesn't exist\n",
    "date = time.strftime('%Y_%m_%d')\n",
    "log_file_name = date + \"_download_log.txt\"\n",
    "log_file_dir = os.path.join(ROOT_DIR + log_file_name)\n",
    "print(log_file_dir)\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    os.mkdir(ROOT_DIR)\n",
    "if os.path.isfile(log_file_dir):\n",
    "    print('log file already exists.')\n",
    "    print('New entries will be appended.')\n",
    "else:\n",
    "    log = open(log_file_dir, \"w\")\n",
    "    log.close()\n",
    "\n",
    "write_to_log(log_file_dir, \"Session started at \" + time.strftime('%Y_%m_%d-%H_%M_%S') + '\\n')\n",
    "\n",
    "# make list of relevant months\n",
    "month_current = MONTH_START\n",
    "months = [MONTH_START]\n",
    "for month in range(MONTH_COUNT-1):\n",
    "    y, m = month_current.split('-')\n",
    "    if m == '12':\n",
    "        m = '01'\n",
    "        y = str(int(y) + 1)\n",
    "    elif int(m) < 9:\n",
    "        m = '0' + str(int(m) + 1)\n",
    "    else:\n",
    "        m = str(int(m) + 1)\n",
    "    month_current = y + '-' + m\n",
    "    months.append(month_current)\n",
    "\n",
    "write_to_log(log_file_dir, f\"Months: {months}\\n\")\n",
    "\n",
    "# get download links for relevant months and sensors\n",
    "for month in months:\n",
    "    # get url\n",
    "    url_curr = URL + month + '/'\n",
    "    print(url_curr)    \n",
    "    write_to_log(log_file_dir, f\"URL: {url_curr}\\n\")\n",
    "\n",
    "    # find download links according to the sensors list and save them with file names\n",
    "    r = requests.get(url_curr)\n",
    "    soup = bs(r.text)\n",
    "    urls = []\n",
    "    names = []\n",
    "    for i, link in enumerate(soup.findAll('a')):\n",
    "        if '.zip' in str(link) and any([sensor in str(link) for sensor in SENSORS]):\n",
    "            url_download = url_curr + link.get('href')\n",
    "            urls.append(url_download)\n",
    "            names.append(soup.select('a')[i].attrs['href'])\n",
    "    print(\"Files to download:\")\n",
    "    for file_name in names:\n",
    "        print(file_name)\n",
    "    write_to_log(log_file_dir, f\"\\tFiles: {names}\\n\")\n",
    "    names_urls = zip(names, urls)\n",
    "\n",
    "    # download files\n",
    "    files_finished = 0\n",
    "    for name, url in names_urls:\n",
    "\n",
    "        # define path where downloaded file will be saved\n",
    "        category = name.split('.')[0].split('_')[-1]\n",
    "        # directory = os.path.join(ROOT_DIR, category, \"\")\n",
    "        directory = ROOT_DIR\n",
    "        full_path = os.path.join(directory, name)\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        \n",
    "        # define path for processed .csv file\n",
    "        processed_dir = os.path.join(ROOT_DIR, \"processed\", \"\")\n",
    "        if not os.path.exists(processed_dir):\n",
    "            os.mkdir(processed_dir)\n",
    "        name_csv = name.split('.')[0] + \".csv\"\n",
    "        csv_processed_dir = os.path.join(processed_dir, name_csv)\n",
    "\n",
    "        # get path of unprocessed .csv file\n",
    "        csv_full = os.path.join(directory, name_csv)\n",
    "\n",
    "        # if the processed .csv file already exists skip download\n",
    "        if os.path.isfile(csv_processed_dir) or os.path.isfile(csv_full) or os.path.isfile(full_path):\n",
    "            if os.path.isfile(csv_processed_dir):\n",
    "                write_to_log(log_file_dir, f\"\\t\\t{csv_processed_dir} already exists... download and processing {name} gets skipped.\\n\")\n",
    "                continue\n",
    "            elif os.path.isfile(csv_full):\n",
    "                write_to_log(log_file_dir, f\"\\t\\t{csv_full} already exists... download of {name} gets skipped.\\n\")\n",
    "            elif os.path.isfile(full_path):\n",
    "                write_to_log(log_file_dir, f\"\\t\\t{full_path} already exists... download of {name} gets skipped.\\n\")\n",
    "\n",
    "        # download .zip file if it doesn't exist yet\n",
    "        if not os.path.isfile(csv_full) and not os.path.isfile(full_path):\n",
    "            print(f\"Start downloading {name}.\")\n",
    "            start = time.time()\n",
    "            response = requests.get(url, timeout=50)\n",
    "            with open(full_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            end = time.time()\n",
    "            print(f\"The download took {round((end - start) / 60, 1)} minutes.\")\n",
    "            write_to_log(log_file_dir, f\"\\t\\t{name}\\n\", f\"\\t\\t\\tDownload successfully finished after {(end - start) / 60} minutes.\\n\")\n",
    "\n",
    "        if os.path.isfile(full_path):\n",
    "            # unzip file\n",
    "            print(\"Unzip file...\")\n",
    "            with zipfile.ZipFile(full_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(directory)\n",
    "            print(\"Unzipping finished\")        \n",
    "            write_to_log(log_file_dir, f\"\\t\\t\\t{name} unzipped\\n\")\n",
    "\n",
    "            # delete .zip\n",
    "            os.remove(full_path)\n",
    "            print(\".zip file deleted\")\n",
    "            write_to_log(log_file_dir, f\"\\t\\t\\t.zip file deleted\\n\")\n",
    "\n",
    "        # define the chunk size that is read from .csv\n",
    "        chunksize = 10 ** 6\n",
    "\n",
    "        # read .csv chunkwise\n",
    "        with pd.read_csv(csv_full, sep=\";\", chunksize=chunksize) as reader:\n",
    "            write_to_log(log_file_dir, f\"\\t\\t\\tprocessing {csv_full}\\n\")\n",
    "            print(f\"processing {csv_full}\\n\")\n",
    "            for i, chunk in enumerate(reader):\n",
    "                # filter data by desired longitude and latitude\n",
    "                for j, lat in enumerate(LAT_RANGE):\n",
    "                    df_temp = chunk[(chunk['lat'] > LAT_RANGE[j][0]) & (chunk['lat'] < LAT_RANGE[j][1]) & (chunk['lon'] > LON_RANGE[j][0]) & (chunk['lon'] < LON_RANGE[j][1])]\n",
    "                    # make a new file for the first chunk and append the subsequent chunks\n",
    "                    if not i and not j:\n",
    "                        df_temp.to_csv(csv_processed_dir, header=True, index=False)\n",
    "                    else:\n",
    "                        df_temp.to_csv(csv_processed_dir, mode='a', header=False, index=False)\n",
    "                    write_to_log(log_file_dir, f\"\\t\\t\\t\\twrote chunk #{i} for region #{j}\\n\")\n",
    "\n",
    "        #delete original .csv file\n",
    "        os.remove(csv_full)\n",
    "        write_to_log(log_file_dir, f\"\\t\\t\\t\\t{csv_full} deleted\\n\")\n",
    "        print(f\"{csv_full} deleted\")\n",
    "\n",
    "        # wait before next download starts\n",
    "        wait = np.random.randint(WAIT_BETWEEN_DOWNLOADS[0], WAIT_BETWEEN_DOWNLOADS[1])\n",
    "        print(f\"Wait for {wait} minutes\")\n",
    "        write_to_log(log_file_dir, f\"\\t\\t\\twait for {wait} minutes\\n\\n\")\n",
    "        time.sleep(wait * 60)\n",
    "        print()\n",
    "\n",
    "script_end = time.time()\n",
    "print(f\"Finished script after {round((script_end - script_start) / 60, 1)} minutes\")\n",
    "write_to_log(log_file_dir, f\"Finished script after {round((script_end - script_start) / 60, 1)} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1d0c01c3ebde60b56da9ded1d66f0f72d104a2bb1824316bb4aff32e9a24f56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
