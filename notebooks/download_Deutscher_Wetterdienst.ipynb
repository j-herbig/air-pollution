{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CDC - Observations Germany](https://opendata.dwd.de/climate_environment/CDC/observations_germany/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind data\n",
    "[Wind historical](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/wind/historical/) </br>\n",
    "[Wind recent](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/wind/recent/) </br></br>\n",
    "[Extreme Wind historical](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/extreme_wind/historical/) </br>\n",
    "[Extreme Wind recent](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/extreme_wind/recent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precipitation data\n",
    "[Precipitation historical](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/precipitation/historical/) </br>\n",
    "[Precipitation recent](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/precipitation/recent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data\n",
    "Run the following script completely to get the desired data from Deutscher Wetterdienst.\n",
    "\n",
    "Information:\n",
    "Since 'recent' only covers the last 520 days, we need to also download 'historical' data in a loop over two periods.\n",
    "\n",
    "1. PERIOD = ['2020-2022', 'recent']\n",
    "2. PERIOD = ['2020 - 2020', 'historical']\n",
    "\n",
    "Desired config parameters:\n",
    "\n",
    "DATA = ['air_temperature', 'pressure', 'precipitation', 'wind'], \n",
    "\n",
    "STATIONS_ID = ['691', '1420'] --> Bremen and Frankfurt a. M.\n",
    "\n",
    "TEMPORAL_RES = ['hourly']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of periods to loop over\n",
    "periods = [['2020 - 2022', 'recent'],\n",
    "        ['2020 - 2020', 'historical']]\n",
    "\n",
    "\n",
    "# not all data are available in every temporal resolution!\n",
    "DATA = [\n",
    "    'air_temperature',\n",
    "    # 'cloud_type',\n",
    "    # 'cloudiness',\n",
    "    # 'dew_point',\n",
    "    #'extreme_wind',\n",
    "    # 'moisture',\n",
    "    'precipitation',\n",
    "    'pressure',\n",
    "    # 'soil',\n",
    "    # 'soil_temperature',\n",
    "    # 'solar',\n",
    "    # 'sun',\n",
    "    # 'standard_format',\n",
    "    # 'visibility',\n",
    "    # 'weather_phenomena',\n",
    "    'wind',\n",
    "    # 'wind_test',\n",
    "    # 'wind_synop',\n",
    "]\n",
    "\n",
    "TEMPORAL_RES = [\n",
    "    # '1_minute',\n",
    "    #'10_minutes',\n",
    "    'hourly',\n",
    "    # 'subdaily',\n",
    "    # 'daily',\n",
    "    # 'monthly',\n",
    "    # 'annual',\n",
    "    # 'multi_annual',\n",
    "]\n",
    "\n",
    "# now dynamically looped over to cover different periods\n",
    "# PERIOD = [\n",
    "#     # 'start - 2020', # in hourly data\n",
    "\n",
    "#     # '1991', # in 10_minutes data\n",
    "#     # '2000 - 2009', # in 10_minutes data\n",
    "#     # '2010 - 2019', # in 10_minutes data\n",
    "#     '2020 - 2020', # in 10_minutes data\n",
    "#     #'recent', \n",
    "#     'historical'\n",
    "# ]\n",
    "\n",
    "STATIONS_ID = [\n",
    "    '691', # Bremen\n",
    "    '1420', # Frankfurt a. M.\n",
    "]\n",
    "\n",
    "ROOT_URL = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/\"\n",
    "\n",
    "DOWNLOAD_DIR = os.path.join(os.curdir, \"../data\", \"DeutscherWetterdienst\", \"\")\n",
    "\n",
    "# make target directory, if it doesn't exist\n",
    "if not os.path.exists(DOWNLOAD_DIR):\n",
    "    os.mkdir(DOWNLOAD_DIR)\n",
    "\n",
    "# ensure that the id has 5 digits\n",
    "for i, s_id in enumerate(STATIONS_ID):\n",
    "    while len(s_id) < 5:\n",
    "        s_id = '0' + s_id\n",
    "    STATIONS_ID[i] = s_id\n",
    "\n",
    "\n",
    "for PERIOD in periods:\n",
    "    # get urls to search for downloadable data\n",
    "    urls_root = []\n",
    "    for temp_res in TEMPORAL_RES:\n",
    "        for dat in DATA:\n",
    "            if 'recent' in PERIOD:\n",
    "                urls_root.append(ROOT_URL + temp_res + '/' + dat + '/' + 'recent' + '/')\n",
    "            if len(PERIOD) > 1 or PERIOD[0] != 'recent':\n",
    "                urls_root.append(ROOT_URL + temp_res + '/' + dat + '/' + 'historical' + '/')\n",
    "\n",
    "    # get relevant years, 'akt' for recent data \n",
    "    years = [y.split(' - ')[1] if len(y.split('-')) > 1 else y.split(' - ')[0] for y in PERIOD]\n",
    "    if 'recent' in PERIOD:\n",
    "        years.append('akt')\n",
    "\n",
    "    # get urls and names of desired files\n",
    "    urls = []\n",
    "    names = []\n",
    "    for url in urls_root:\n",
    "        # get html of website\n",
    "        r = requests.get(url)\n",
    "        soup = bs(r.text)\n",
    "        # find download links and filter for .zip files, station and relevant time periods\n",
    "        for i, link in enumerate(soup.findAll('a')):\n",
    "            if '.zip' in str(link) and \\\n",
    "                any([station in str(link) for station in STATIONS_ID]) and \\\n",
    "                    any([year in str(link) for year in years]):\n",
    "                url_download = url + link.get('href')\n",
    "                urls.append(url_download)\n",
    "                names.append(soup.select('a')[i].attrs['href'])\n",
    "\n",
    "    names_urls = zip(names, urls)\n",
    "\n",
    "    # download files\n",
    "    for name, url in names_urls:\n",
    "        \n",
    "        file_path = os.path.join(DOWNLOAD_DIR, name)\n",
    "        file_path_txt = os.path.join(DOWNLOAD_DIR, name.split('.')[0] + '.txt')\n",
    "        if not os.path.isfile(file_path) and not os.path.isfile(file_path_txt):\n",
    "            response = requests.get(url, timeout=50)\n",
    "            print(url)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            # unzip file\n",
    "            if os.path.isfile(file_path):\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(DOWNLOAD_DIR)\n",
    "\n",
    "        # delete .zip\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combine data of cities and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all meteorological data for a city from 'data/DeutscherWetterdienst'\n",
    "def read_city_data(city_codes):\n",
    "    \"\"\"Reads all data of different cities and metrics\n",
    "\n",
    "    Args:\n",
    "        city_codes (list): list of city codes to be processed\n",
    "\n",
    "    Returns:\n",
    "        dictionary: dictionary of different dataframes per metric, city code and period (recent - 2 or historic - 1)\n",
    "    \"\"\"\n",
    "    path = r'../data/DeutscherWetterdienst' \n",
    "    all_files = glob.glob(path + \"/produkt*.txt\") \n",
    "\n",
    "    weather_metrics = {}    \n",
    "    for filename in all_files: \n",
    "            # loop over all cities in parameterization\n",
    "            for city_code in city_codes:\n",
    "                if city_code in filename:\n",
    "                    df = pd.read_csv(filename, sep=';')\n",
    "                    df['date'] = pd.to_datetime(df.MESS_DATUM, format='%Y%m%d%H')\n",
    "                    df.drop(['eor', 'MESS_DATUM', df.columns[df.columns.str.startswith('QN')][0]], axis=1, inplace=True)\n",
    "                    df = df.query(\"date < '2022-04' and date >= '2020-01'\")\n",
    "                    # keys of dictionary consist of measurement name, city code and first digit of period YYYY (1 or 2)\n",
    "                    weather_metrics[f'{filename[38:40] + filename[-9:-4] + filename[48]}'] = df\n",
    "    return weather_metrics\n",
    "\n",
    "# Concatenate historical and current data for each metric\n",
    "def concat_city_data(metrics_dict):\n",
    "    \"\"\"Concatenate all data per metric\n",
    "\n",
    "    Args:\n",
    "        metrics_dict (dictionary): dictionary of dataframes per metric, city code and period (recent - 2 or historic - 1)\n",
    "\n",
    "    Returns:\n",
    "        list: list of dataframes (one per metric), containing historical and current data for all cities\n",
    "    \"\"\"\n",
    "    concatenated_metrics = []\n",
    "    # loop over different metrics\n",
    "    for metric in set([x[:-6] for x in list(metrics_dict.keys())]):\n",
    "        # get all corresponding keys for one metric (includes recent and historic data and all cities)\n",
    "        metric_keys = [x for x in list(metrics_dict.keys()) if metric in x]\n",
    "        # create list of all dataframes for one metric (for all cities)\n",
    "        metric_dfs = [metrics_dict[x] for x in metric_keys]\n",
    "        concatenated_metrics.append(pd.concat(metric_dfs, axis=0, ignore_index=True).drop_duplicates(keep='first'))\n",
    "    return concatenated_metrics\n",
    "\n",
    "\n",
    "# Merge all meterological data into one dataframe for all cities and metrics\n",
    "def merge_city_data(df_list):\n",
    "    \"\"\"Merge all data into one dataframe for different metrics\n",
    "\n",
    "    Args:\n",
    "        df_list (list): list of dataframes for different metrics\n",
    "\n",
    "    Returns:\n",
    "        dataframe: one dataframe with all metrics\n",
    "    \"\"\"\n",
    "    df_merged = df_list[0]\n",
    "    for df in df_list[1:]:\n",
    "        df_merged = df_merged.merge(df, on=['date', 'STATIONS_ID'], how='left')\n",
    "    df_merged.sort_values('date', ascending=True, inplace=True)\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def label_cities(stations_id):\n",
    "    \"\"\"Create label from stations_id\n",
    "\n",
    "    Args:\n",
    "        stations_id (string): string of stations_id for city\n",
    "\n",
    "    Returns:\n",
    "        string: corresponding label for given stations_id\n",
    "    \"\"\"\n",
    "    if (stations_id==1420):\n",
    "        return 'Frankfurt'\n",
    "    elif (stations_id==691):\n",
    "        return 'Bremen'\n",
    "    # add new cities here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for all cities (global variable STATIONS_ID) and metrics\n",
    "# append the three functions defined above and call them with parameter for list of stations_ids\n",
    "dwd_all_cities = merge_city_data(concat_city_data(read_city_data(STATIONS_ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add city column (e.g. Frankfurt, Bremen)\n",
    "dwd_all_cities['city'] = dwd_all_cities.apply(lambda x: label_cities(x.STATIONS_ID), axis=1)\n",
    "# drop column STATIONS_ID because it is no longer needed\n",
    "# drop PrecipitationIndicator\n",
    "dwd_all_cities.drop(['STATIONS_ID', 'RS_IND'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns into readable format\n",
    "dwd_all_cities = dwd_all_cities.rename(columns = {'RF_TU': 'humidity', 'TT_TU': 'temperature', '  R1': 'precip', '   F': 'wind_speed', '   D': 'wind_direction', '   P': 'pressure_sealevel', '  P0': 'pressure'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwd_all_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "import datetime\n",
    "day = datetime.datetime.now().date()\n",
    "dwd_all_cities.to_csv(f'../data/processed_deutscher_wetterdienst_{day}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1d0c01c3ebde60b56da9ded1d66f0f72d104a2bb1824316bb4aff32e9a24f56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
